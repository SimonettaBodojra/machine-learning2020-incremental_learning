{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/danielegenta/Progetto-MLDL/blob/master/main_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Import libraries**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATASET_ROOT = 'cifar-100-python'\n",
    "import os\n",
    "if not os.path.isdir(DATASET_ROOT):\n",
    "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
    "    !tar -xf 'cifar-100-python.tar.gz'  \n",
    "    !rm -rf 'cifar-100-python.tar.gz'\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.backends import cudnn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from cifar100 import split_train_validation\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SET ARGUMENTS**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "arguments = utils.get_arguments()\n",
    "\n",
    "DEVICE = arguments['DEVICE']\n",
    "NUM_CLASSES = arguments[\"NUM_CLASSES\"] \n",
    "\n",
    "BATCH_SIZE = arguments[\"BATCH_SIZE\"]        # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                                            # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR = arguments[\"LR\"]                        # The initial Learning Rate\n",
    "MOMENTUM = arguments[\"MOMENTUM\"]            # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = arguments[\"WEIGHT_DECAY\"]    # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = arguments[\"NUM_EPOCHS\"]        # Total number of training epochs (iterations over dataset)\n",
    "GAMMA = arguments[\"GAMMA\"]                  # Multiplicative factor for learning rate step-down\n",
    "\n",
    "LOG_FREQUENCY = arguments[\"LOG_FREQUENCY\"]\n",
    "MILESTONES = arguments[\"MILESTONES\"]\n",
    "SEED = arguments[\"SEED\"]\n",
    "\n",
    "OUTPUT_PATH = \"RUN1\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Define Data Preprocessing**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_transforms, eval_transforms = utils.get_train_eval_transforms()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Prepare Dataset**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_val_dataset = utils.get_cifar_with_seed(DATASET_ROOT, train_transforms, src='train', seed=SEED)\n",
    "test_dataset = utils.get_cifar_with_seed(DATASET_ROOT, train_transforms, src='test', seed=SEED)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Prepare Network**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net, criterion, optimizer, scheduler = utils.get_resnet(LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA, resnet=32)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Train, Test, Validation functions**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_batch(net, train_loader, criterion, optimizer, current_step, device=DEVICE):\n",
    "    \n",
    "    cumulative_loss =.0\n",
    "    running_corrects = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        net.train()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        \n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        cumulative_loss += loss.item()\n",
    "        \n",
    "        if current_step % LOG_FREQUENCY == 0:\n",
    "                print('\\t\\tTrain step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return cumulative_loss / len(train_loader), running_corrects, current_step\n",
    "\n",
    "def validate(net, val_loader, criterion, optimizer, device=DEVICE):\n",
    "    cumulative_loss =.0\n",
    "    running_corrects = 0\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        net.eval()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        \n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        cumulative_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return cumulative_loss / len(val_loader), running_corrects\n",
    "\n",
    "def test(net, test_loader, device=DEVICE):\n",
    "    \n",
    "    running_corrects = 0\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        net.eval()\n",
    "        outputs = net(images)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "\n",
    "    return running_corrects\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**FINE TUNING FUNCTION**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def fine_tuning(net, train_val_dataset, test_dataset, criterion, optimizer, \n",
    "                scheduler, max_epoch=NUM_EPOCHS, file_path=OUTPUT_PATH, device=DEVICE):\n",
    "    import math\n",
    "    incremental_test = []\n",
    "    train_mean_stage_accuracies = []\n",
    "    val_mean_stage_accuracies = []\n",
    "    test_stage_accuracies = []\n",
    "    net = net.to(device)\n",
    "    cudnn.benchmark\n",
    "    for stage in range(10):\n",
    "        print(f\"STARTING FINE TUNING STAGE {stage+1}...\\n\\n\\n\")\n",
    "        # Get indices\n",
    "        # 4000 training, 1000 validation\n",
    "        train_idx, val_idx, test_idx = utils.get_kth_batch(train_val_dataset, test_dataset, stage,\n",
    "                                                                 seed=SEED, train_size=.80, get='indices')\n",
    "        \n",
    "        # Make test set incremental\n",
    "        incremental_test.extend(test_idx)\n",
    "        train_set, val_set, test_set = Subset(train_val_dataset, train_idx),\\\n",
    "                                               Subset(train_val_dataset, val_idx),\\\n",
    "                                               Subset(test_dataset, incremental_test)\n",
    "        # Build data loaders\n",
    "        curr_train_loader = utils.get_train_loader(train_set,batch_size=BATCH_SIZE)\n",
    "        curr_val_loader = utils.get_eval_loader(val_set, batch_size=BATCH_SIZE)\n",
    "        curr_test_loader = utils.get_eval_loader(test_set, batch_size=BATCH_SIZE)\n",
    "        \n",
    "        # Init results\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        min_val_loss = 0\n",
    "        current_step = 0\n",
    "        tolerance = 3\n",
    "        for epoch in range(max_epoch):\n",
    "            print(f\"\\tSTARTING EPOCH {epoch+1}......\")\n",
    "            curr_result = train_batch(net, curr_train_loader, criterion, optimizer, current_step, device)\n",
    "            curr_train_loss = curr_result[0]\n",
    "            curr_train_accuracy = curr_result[1] / float(len(BATCH_SIZE * len(curr_train_loader)))\n",
    "            current_step = curr_result[2]\n",
    "            \n",
    "            train_losses.append(curr_train_loss)\n",
    "            train_accuracies.append(curr_train_accuracy)\n",
    "            scheduler.step()\n",
    "            \n",
    "            curr_val_loss, val_corrects = validate(net, curr_val_loader, criterion, optimizer, device)\n",
    "            val_losses.append(curr_val_loss)\n",
    "            curr_val_accuracy = val_corrects / float(len(val_set))\n",
    "            val_accuracies.append(curr_val_accuracy)\n",
    "            \n",
    "            print(f\"\\t\\tRESULT EPOCH {epoch+1}:\")\n",
    "            print(f\"\\t\\t\\tTrain Loss: {curr_train_loss} - Train Accuracy: {curr_train_accuracy}\")\n",
    "            print(f\"\\t\\t\\tVal Loss: {curr_val_loss} - Val Accuracy: {curr_val_accuracy}\")\n",
    "            \n",
    "            if math.isnan(curr_val_loss):\n",
    "                tolerance -= 1\n",
    "            else:\n",
    "                tolerance = 3\n",
    "            \n",
    "            if tolerance == 0:\n",
    "                print(f\"STAGE {stage+1} -> EARLY STOPPING\")\n",
    "                break\n",
    "            \n",
    "            if min_val_loss > curr_val_loss:\n",
    "                min_val_loss = curr_val_loss\n",
    "                torch.save(net, f\"{file_path}_best_model.pth\")\n",
    "        \n",
    "        net = torch.load(f\"{file_path}_best_model_finetuning.pth\").to(device)\n",
    "        epoch_test_accuracy = test(net, curr_test_loader, device) / float(len(test_set))\n",
    "        test_stage_accuracies.append(epoch_test_accuracy)\n",
    "        train_mean_stage_accuracies.append(np.mean(train_accuracies))\n",
    "        val_mean_stage_accuracies.append(np.mean(val_accuracies))\n",
    "        \n",
    "        print(f\"\\tResults STAGE {stage+1}:\")\n",
    "        print(f\"\\t\\tTrain Mean Accuracy: {train_mean_stage_accuracies[stage]}\")\n",
    "        print(f\"\\t\\tVal Mean Accuracy: {val_mean_stage_accuracies[stage]}\")\n",
    "        print(f\"\\t\\tTest Accuracy: {test_stage_accuracies[stage]}\")\n",
    "        \n",
    "    return train_mean_stage_accuracies,\\\n",
    "           val_mean_stage_accuracies,\\\n",
    "           test_stage_accuracies"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**FINE TUNING START**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_accuracies,\\\n",
    "val_accuracies,\\\n",
    "test_accuracies = fine_tuning(net, train_val_dataset, test_dataset,\n",
    "                              criterion, optimizer, scheduler, NUM_EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}