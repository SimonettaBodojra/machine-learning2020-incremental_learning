{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "colab": {
      "name": "IL_fine_tuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "tvETQMX1ipNf",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1Df_YvI2mdf9SoeA1GZLecH_3_mthCWei\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab Account AI\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-LoM_h1IXAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93a3e3ea-7928-4f5f-a520-c6c015704869"
      },
      "source": [
        "\"\"\"# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\"\"\"\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "print(gpu.name)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla P100-PCIE-16GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "wwN82ZV7ipNg",
        "colab_type": "text"
      },
      "source": [
        "**Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "RSnex0bmipNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATASET_ROOT = 'cifar-100-python'\n",
        "CODE_ROOT = 'libs'\n",
        "import os\n",
        "if not os.path.isdir(DATASET_ROOT):\n",
        "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "    !tar -xf 'cifar-100-python.tar.gz'  \n",
        "    !rm -rf 'cifar-100-python.tar.gz'\n",
        "\n",
        "if not os.path.isdir(CODE_ROOT):\n",
        "  !git clone https://lore-lml:29f601e814e0446c5b17a9f6c3684d1cbd316bcf@github.com/lore-lml/machine-learning2020-incremental_learning.git\n",
        "  !mv 'machine-learning2020-incremental_learning/libs' '.'\n",
        "  !rm -rf 'machine-learning2020-incremental_learning'\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Subset, DataLoader\n",
        "from torch.backends import cudnn\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import libs.utils as utils\n",
        "from libs.utils import one_hot_encode_labels\n",
        "\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "7W9y67yoipNk",
        "colab_type": "text"
      },
      "source": [
        "**SET ARGUMENTS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "r0hjWAP3ipNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "arguments = utils.get_arguments()\n",
        "\n",
        "DEVICE = arguments['DEVICE']\n",
        "NUM_CLASSES = arguments[\"NUM_CLASSES\"] \n",
        "\n",
        "BATCH_SIZE = arguments[\"BATCH_SIZE\"]        # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
        "                                            # the batch size, learning rate should change by the same factor to have comparable results\n",
        "\n",
        "LR = arguments[\"LR\"]                        # The initial Learning Rate\n",
        "MOMENTUM = arguments[\"MOMENTUM\"]            # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
        "WEIGHT_DECAY = arguments[\"WEIGHT_DECAY\"]    # Regularization, you can keep this at the default\n",
        "\n",
        "NUM_EPOCHS = arguments[\"NUM_EPOCHS\"]        # Total number of training epochs (iterations over dataset)\n",
        "GAMMA = arguments[\"GAMMA\"]                  # Multiplicative factor for learning rate step-down\n",
        "\n",
        "LOG_FREQUENCY = arguments[\"LOG_FREQUENCY\"]\n",
        "MILESTONES = arguments[\"MILESTONES\"]\n",
        "SEED = arguments[\"SEED\"]\n",
        "\n",
        "LOSS_TYPE = 'bce'\n",
        "\n",
        "# TRAINING_TYPE = 'FT'\n",
        "TRAINING_TYPE = 'JT'\n",
        "OUTPUT_PATH = f\"RUN1_{TRAINING_TYPE}\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "SaT8eFDNipNm",
        "colab_type": "text"
      },
      "source": [
        "**Define Data Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "m-ydAGw4ipNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_transforms, eval_transforms = utils.get_train_eval_transforms()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "X7Naz_DdipNp",
        "colab_type": "text"
      },
      "source": [
        "**Prepare Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "G-Xct5sNipNp",
        "colab_type": "code",
        "outputId": "7120c3a3-82de-4e31-83f5-3917cfa46802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "train_val_dataset = utils.get_cifar_with_seed(DATASET_ROOT, train_transforms, src='train', seed=SEED)\n",
        "test_dataset = utils.get_cifar_with_seed(DATASET_ROOT, eval_transforms, src='test', seed=SEED)\n",
        "\n",
        "print(f\"Size Training Set: {len(train_val_dataset)}\")\n",
        "print(f\"Size Test Set: {len(test_dataset)}\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size Training Set: 50000\n",
            "Size Test Set: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "xZDP5yXBipNt",
        "colab_type": "text"
      },
      "source": [
        "**Train, Test, Validation functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "secPALBtipNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_batch(net, train_loader, criterion, optimizer, current_step, device=DEVICE):\n",
        "    net.train()\n",
        "    cumulative_loss =.0\n",
        "    running_corrects = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        if LOSS_TYPE == 'bce':\n",
        "            labels_enc = one_hot_encode_labels(labels).to(device)\n",
        "\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(images)\n",
        "        \n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "        \n",
        "        loss = criterion(outputs, labels_enc) if LOSS_TYPE == 'bce'\\\n",
        "                                              else criterion(outputs, labels)\n",
        "        loss = criterion(outputs, labels_enc)\n",
        "        cumulative_loss += loss.item()\n",
        "        \n",
        "        if current_step != 0 and current_step % LOG_FREQUENCY == 0:\n",
        "                print('\\t\\tTrain step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        current_step += 1\n",
        "\n",
        "    return cumulative_loss / len(train_loader), running_corrects, current_step\n",
        "\n",
        "def validate(net, val_loader, criterion, optimizer, device=DEVICE):\n",
        "    net.eval()\n",
        "    cumulative_loss =.0\n",
        "    running_corrects = 0\n",
        "    for images, labels in val_loader:\n",
        "        images = images.to(device)\n",
        "\n",
        "        if LOSS_TYPE == 'bce':\n",
        "            labels_enc = one_hot_encode_labels(labels).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = net(images)\n",
        "        \n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "        \n",
        "        loss = criterion(outputs, labels_enc) if LOSS_TYPE == 'bce'\\\n",
        "                                              else criterion(outputs, labels)\n",
        "        cumulative_loss += loss.item()\n",
        "\n",
        "\n",
        "    return cumulative_loss / len(val_loader), running_corrects\n",
        "\n",
        "def test(net, test_loader, device=DEVICE):\n",
        "    \n",
        "    running_corrects = 0\n",
        "    for images, labels in tqdm(test_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        net.eval()\n",
        "        outputs = net(images)\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
        "\n",
        "    return running_corrects\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "s5SroLpaipNw",
        "colab_type": "text"
      },
      "source": [
        "**FINE TUNING FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "clnGi_eLipNw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fine_tuning(train_val_dataset, test_dataset, max_epoch=NUM_EPOCHS, file_path=OUTPUT_PATH, device=DEVICE):\n",
        "    import math, time\n",
        "    incremental_test = []\n",
        "    train_mean_stage_accuracies = []\n",
        "    val_mean_stage_accuracies = []\n",
        "    test_stage_accuracies = []\n",
        "    cudnn.benchmark\n",
        "    net = utils.get_resnet(32).to(device)\n",
        "    criterion = utils.get_criterion(LOSS_TYPE)\n",
        "    start_time = time.time()\n",
        "    for stage in range(10):\n",
        "        optimizer, scheduler = utils.get_otpmizer_scheduler(net.parameters(), LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA)\n",
        "        print(f\"STARTING FINE TUNING STAGE {stage+1}...\")\n",
        "        # Get indices\n",
        "        # 4000 training, 1000 validation\n",
        "        train_idx, val_idx, test_idx = utils.get_kth_batch(train_val_dataset, test_dataset, stage,\n",
        "                                                                 seed=SEED, train_size=.9, get='indices')\n",
        "        \n",
        "        # Make test set incremental\n",
        "        incremental_test.extend(test_idx)\n",
        "        train_set, val_set, test_set = Subset(train_val_dataset, train_idx),\\\n",
        "                                       Subset(train_val_dataset, val_idx),\\\n",
        "                                       Subset(test_dataset, incremental_test)\n",
        "\n",
        "        # Build data loaders\n",
        "        curr_train_loader = utils.get_train_loader(train_set,batch_size=BATCH_SIZE)\n",
        "        curr_val_loader = utils.get_eval_loader(val_set, batch_size=BATCH_SIZE)\n",
        "        curr_test_loader = utils.get_eval_loader(test_set, batch_size=BATCH_SIZE)\n",
        "\n",
        "        # Init results\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_accuracies = []\n",
        "        val_accuracies = []\n",
        "        min_val_loss = -1\n",
        "        current_step = 0\n",
        "        tolerance = 10\n",
        "        for epoch in range(max_epoch):\n",
        "            print(f\"\\tSTARTING EPOCH {epoch+1} - LR={scheduler.get_last_lr()}...\")\n",
        "            curr_result = train_batch(net, curr_train_loader, criterion, optimizer, current_step, device)\n",
        "            curr_train_loss = curr_result[0]\n",
        "            curr_train_accuracy = curr_result[1] / float(BATCH_SIZE * len(curr_train_loader))\n",
        "            current_step = curr_result[2]\n",
        "            \n",
        "            train_losses.append(curr_train_loss)\n",
        "            train_accuracies.append(curr_train_accuracy)\n",
        "            scheduler.step()\n",
        "            \n",
        "            curr_val_loss, val_corrects = validate(net, curr_val_loader, criterion, optimizer, device)\n",
        "            val_losses.append(curr_val_loss)\n",
        "            curr_val_accuracy = val_corrects / float(len(val_set))\n",
        "            val_accuracies.append(curr_val_accuracy)\n",
        "            \n",
        "            print(f\"\\t\\tRESULT EPOCH {epoch+1}:\")\n",
        "            print(f\"\\t\\t\\tTrain Loss: {curr_train_loss} - Train Accuracy: {curr_train_accuracy}\")\n",
        "            print(f\"\\t\\t\\tVal Loss: {curr_val_loss} - Val Accuracy: {curr_val_accuracy}\\n\")\n",
        "            \n",
        "            if math.isnan(curr_val_loss):\n",
        "                tolerance -= 1\n",
        "            else:\n",
        "                tolerance = 10\n",
        "            \n",
        "            if tolerance == 0:\n",
        "                print(f\"STAGE {stage+1} -> EARLY STOPPING\\n\")\n",
        "                break\n",
        "            \n",
        "            if min_val_loss == -1 or min_val_loss > curr_val_loss:\n",
        "                min_val_loss = curr_val_loss\n",
        "                torch.save(net, f\"{file_path}_best_model_finetuning.pth\")\n",
        "        \n",
        "        net = torch.load(f\"{file_path}_best_model_finetuning.pth\").to(device)\n",
        "        epoch_test_accuracy = test(net, curr_test_loader, device) / float(len(test_set))\n",
        "        test_stage_accuracies.append(epoch_test_accuracy)\n",
        "        train_mean_stage_accuracies.append(np.mean(train_accuracies))\n",
        "        val_mean_stage_accuracies.append(np.mean(val_accuracies))\n",
        "        \n",
        "        print(f\"\\n\\tResults STAGE {stage+1}:\")\n",
        "        print(f\"\\t\\tTrain Mean Accuracy: {train_mean_stage_accuracies[stage]}\")\n",
        "        print(f\"\\t\\tVal Mean Accuracy: {val_mean_stage_accuracies[stage]}\")\n",
        "        print(f\"\\t\\tTest Accuracy: {test_stage_accuracies[stage]}\\n\")\n",
        "\n",
        "\n",
        "    total_time = int(time.time() - start_time)\n",
        "    min = int(total_time / 60)\n",
        "    sec = total_time % 60\n",
        "    print(f\"\\nTotal time: {min} min {sec} sec\\n\")\n",
        "        \n",
        "    return train_mean_stage_accuracies,\\\n",
        "           val_mean_stage_accuracies,\\\n",
        "           test_stage_accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sLnI2Dqx6I-",
        "colab_type": "text"
      },
      "source": [
        "**JOINT TRAINING FUNCTION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44iaChR_yM34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def joint_training(train_val_dataset, test_dataset, max_epoch=NUM_EPOCHS, file_path=OUTPUT_PATH, device=DEVICE):\n",
        "    import math, time\n",
        "    incremental_train = []\n",
        "    incremental_val = []\n",
        "    incremental_test = []\n",
        "    train_mean_stage_accuracies = []\n",
        "    val_mean_stage_accuracies = []\n",
        "    test_stage_accuracies = []\n",
        "    cudnn.benchmark\n",
        "    net = utils.get_resnet(32).to(device)\n",
        "    criterion = utils.get_criterion(LOSS_TYPE)\n",
        "    start_time = time.time()\n",
        "    for stage in range(10):\n",
        "        optimizer, scheduler = utils.get_otpmizer_scheduler(net.parameters(), LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA)\n",
        "        print(f\"STARTING JOINT TRAINING STAGE {stage+1}...\")\n",
        "        # Get indices\n",
        "        # 4000 training, 1000 validation\n",
        "        train_idx, val_idx, test_idx = utils.get_kth_batch(train_val_dataset, test_dataset, stage,\n",
        "                                                                 seed=SEED, train_size=.9, get='indices')\n",
        "        \n",
        "        # Make test set incremental\n",
        "        incremental_train.extend(train_idx)\n",
        "        incremental_val.extend(val_idx)\n",
        "        incremental_test.extend(test_idx)\n",
        "        train_set, val_set, test_set = Subset(train_val_dataset, incremental_train),\\\n",
        "                                       Subset(train_val_dataset, incremental_val),\\\n",
        "                                       Subset(test_dataset, incremental_test)\n",
        "\n",
        "        print(len(set(incremental_test)))\n",
        "\n",
        "        # Build data loaders\n",
        "        curr_train_loader = utils.get_train_loader(train_set,batch_size=BATCH_SIZE)\n",
        "        curr_val_loader = utils.get_eval_loader(val_set, batch_size=BATCH_SIZE)\n",
        "        curr_test_loader = utils.get_eval_loader(test_set, batch_size=BATCH_SIZE)\n",
        "\n",
        "        # Init results\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_accuracies = []\n",
        "        val_accuracies = []\n",
        "        min_val_loss = -1\n",
        "        current_step = 0\n",
        "        tolerance = 10\n",
        "        for epoch in range(max_epoch):\n",
        "            print(f\"\\tSTARTING EPOCH {epoch+1} - LR={scheduler.get_last_lr()}...\")\n",
        "            curr_result = train_batch(net, curr_train_loader, criterion, optimizer, current_step, device)\n",
        "            curr_train_loss = curr_result[0]\n",
        "            curr_train_accuracy = curr_result[1] / float(BATCH_SIZE * len(curr_train_loader))\n",
        "            current_step = curr_result[2]\n",
        "            \n",
        "            train_losses.append(curr_train_loss)\n",
        "            train_accuracies.append(curr_train_accuracy)\n",
        "            scheduler.step()\n",
        "            \n",
        "            curr_val_loss, val_corrects = validate(net, curr_val_loader, criterion, optimizer, device)\n",
        "            val_losses.append(curr_val_loss)\n",
        "            curr_val_accuracy = val_corrects / float(len(val_set))\n",
        "            val_accuracies.append(curr_val_accuracy)\n",
        "            \n",
        "            print(f\"\\t\\tRESULT EPOCH {epoch+1}:\")\n",
        "            print(f\"\\t\\t\\tTrain Loss: {curr_train_loss} - Train Accuracy: {curr_train_accuracy}\")\n",
        "            print(f\"\\t\\t\\tVal Loss: {curr_val_loss} - Val Accuracy: {curr_val_accuracy}\\n\")\n",
        "            \n",
        "            if math.isnan(curr_val_loss):\n",
        "                tolerance -= 1\n",
        "            else:\n",
        "                tolerance = 10\n",
        "            \n",
        "            if tolerance == 0:\n",
        "                print(f\"STAGE {stage+1} -> EARLY STOPPING\\n\")\n",
        "                break\n",
        "            \n",
        "            if min_val_loss == -1 or min_val_loss > curr_val_loss:\n",
        "                min_val_loss = curr_val_loss\n",
        "                torch.save(net, f\"{file_path}_best_model_finetuning.pth\")\n",
        "        \n",
        "        net = torch.load(f\"{file_path}_best_model_finetuning.pth\").to(device)\n",
        "        epoch_test_accuracy = test(net, curr_test_loader, device) / float(len(test_set))\n",
        "        test_stage_accuracies.append(epoch_test_accuracy)\n",
        "        train_mean_stage_accuracies.append(np.mean(train_accuracies))\n",
        "        val_mean_stage_accuracies.append(np.mean(val_accuracies))\n",
        "        \n",
        "        print(f\"\\n\\tResults STAGE {stage+1}:\")\n",
        "        print(f\"\\t\\tTrain Mean Accuracy: {train_mean_stage_accuracies[stage]}\")\n",
        "        print(f\"\\t\\tVal Mean Accuracy: {val_mean_stage_accuracies[stage]}\")\n",
        "        print(f\"\\t\\tTest Accuracy: {test_stage_accuracies[stage]}\\n\")\n",
        "\n",
        "\n",
        "    total_time = int(time.time() - start_time)\n",
        "    min = int(total_time / 60)\n",
        "    sec = total_time % 60\n",
        "    print(f\"\\nJOINT TRAININGTotal time: {min} min {sec} sec\\n\")\n",
        "        \n",
        "    return train_mean_stage_accuracies,\\\n",
        "           val_mean_stage_accuracies,\\\n",
        "           test_stage_accuracies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "bvaYg8SiipNy",
        "colab_type": "text"
      },
      "source": [
        "**FINE TUNING START**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "i_ejvvl4ipNy",
        "colab_type": "code",
        "outputId": "8ad9c1a3-6305-4e7d-9d5e-e329a81b91ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "train_accuracies,\\\n",
        "val_accuracies,\\\n",
        "test_accuracies = fine_tuning(train_val_dataset, test_dataset, NUM_EPOCHS) if TRAINING_TYPE == 'FT' else joint_training(train_val_dataset, test_dataset, NUM_EPOCHS)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "STARTING JOINT TRAINING STAGE 1...\n",
            "1000\n",
            "STARTING JOINT TRAINING STAGE 2...\n",
            "2000\n",
            "STARTING JOINT TRAINING STAGE 3...\n",
            "3000\n",
            "STARTING JOINT TRAINING STAGE 4...\n",
            "4000\n",
            "STARTING JOINT TRAINING STAGE 5...\n",
            "5000\n",
            "STARTING JOINT TRAINING STAGE 6...\n",
            "6000\n",
            "STARTING JOINT TRAINING STAGE 7...\n",
            "7000\n",
            "STARTING JOINT TRAINING STAGE 8...\n",
            "8000\n",
            "STARTING JOINT TRAINING STAGE 9...\n",
            "9000\n",
            "STARTING JOINT TRAINING STAGE 10...\n",
            "10000\n",
            "\n",
            "JOINT TRAININGTotal time: 0 min 0 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciGlvEWabcbD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "bdd55d30-339b-4122-ccd4-d4abcf338ef7"
      },
      "source": [
        "plt.figure()\n",
        "class_batches = np.arange(10, 101, 10)\n",
        "plt.figure()\n",
        "plt.scatter(class_batches, np.array(test_accuracies)*100, zorder = 100, c='orange')\n",
        "plt.plot(class_batches, np.array(test_accuracies)*100)\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-8f833fdc320a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mclass_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m101\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzorder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'orange'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, data, **kwargs)\u001b[0m\n\u001b[1;32m   2814\u001b[0m         \u001b[0mverts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0medgecolors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2815\u001b[0m         plotnonfinite=plotnonfinite, **({\"data\": data} if data is not\n\u001b[0;32m-> 2816\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2817\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2818\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mscatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4378\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4380\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must be the same size\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}