{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "colab": {
   "name": "IL_fine_tuning.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "tvETQMX1ipNf",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1Df_YvI2mdf9SoeA1GZLecH_3_mthCWei\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab Account AI\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5-LoM_h1IXAi",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "93a3e3ea-7928-4f5f-a520-c6c015704869"
   },
   "source": [
    "\"\"\"# memory footprint support libraries/code\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
    "gpu = GPUs[0]\n",
    "print(gpu.name)\"\"\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "wwN82ZV7ipNg",
    "colab_type": "text"
   },
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "RSnex0bmipNh",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "DATASET_ROOT = 'cifar-100-python'\n",
    "CODE_ROOT = 'libs'\n",
    "import os\n",
    "if not os.path.isdir(DATASET_ROOT):\n",
    "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
    "    !tar -xf 'cifar-100-python.tar.gz'  \n",
    "    !rm -rf 'cifar-100-python.tar.gz'\n",
    "\n",
    "if not os.path.isdir(CODE_ROOT):\n",
    "  !git clone https://lore-lml:29f601e814e0446c5b17a9f6c3684d1cbd316bcf@github.com/lore-lml/machine-learning2020-incremental_learning.git\n",
    "  !mv 'machine-learning2020-incremental_learning/libs' '.'\n",
    "  !rm -rf 'machine-learning2020-incremental_learning'\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torch.backends import cudnn\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import libs.utils as utils\n",
    "from libs.utils import one_hot_encode_labels\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "7W9y67yoipNk",
    "colab_type": "text"
   },
   "source": [
    "**SET ARGUMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "r0hjWAP3ipNk",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "\n",
    "\n",
    "arguments = utils.get_arguments()\n",
    "\n",
    "DEVICE = arguments['DEVICE']\n",
    "NUM_CLASSES = arguments[\"NUM_CLASSES\"] \n",
    "\n",
    "BATCH_SIZE = arguments[\"BATCH_SIZE\"]        # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                                            # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR = arguments[\"LR\"]                        # The initial Learning Rate\n",
    "MOMENTUM = arguments[\"MOMENTUM\"]            # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = arguments[\"WEIGHT_DECAY\"]    # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = arguments[\"NUM_EPOCHS\"]        # Total number of training epochs (iterations over dataset)\n",
    "GAMMA = arguments[\"GAMMA\"]                  # Multiplicative factor for learning rate step-down\n",
    "\n",
    "LOG_FREQUENCY = arguments[\"LOG_FREQUENCY\"]\n",
    "MILESTONES = arguments[\"MILESTONES\"]\n",
    "SEED = arguments[\"SEED\"]\n",
    "\n",
    "LOSS_TYPE = 'bce'\n",
    "\n",
    "# TRAINING_TYPE = 'FT'\n",
    "TRAINING_TYPE = 'JT'\n",
    "OUTPUT_PATH = f\"RUN1_{TRAINING_TYPE}\""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "SaT8eFDNipNm",
    "colab_type": "text"
   },
   "source": [
    "**Define Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "m-ydAGw4ipNm",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "train_transforms, eval_transforms = utils.get_train_eval_transforms()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "X7Naz_DdipNp",
    "colab_type": "text"
   },
   "source": [
    "**Prepare Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "G-Xct5sNipNp",
    "colab_type": "code",
    "outputId": "7120c3a3-82de-4e31-83f5-3917cfa46802",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    }
   },
   "source": [
    "train_val_dataset = utils.get_cifar_with_seed(DATASET_ROOT, train_transforms, src='train', seed=SEED)\n",
    "test_dataset = utils.get_cifar_with_seed(DATASET_ROOT, eval_transforms, src='test', seed=SEED)\n",
    "\n",
    "print(f\"Size Training Set: {len(train_val_dataset)}\")\n",
    "print(f\"Size Test Set: {len(test_dataset)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "xZDP5yXBipNt",
    "colab_type": "text"
   },
   "source": [
    "**Train, Test, Validation functions**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "secPALBtipNt",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def train_batch(net, train_loader, criterion, optimizer, current_step, device=DEVICE):\n",
    "    net.train()\n",
    "    cumulative_loss =.0\n",
    "    running_corrects = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        if LOSS_TYPE == 'bce':\n",
    "            labels_enc = one_hot_encode_labels(labels).to(device)\n",
    "\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        \n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "        \n",
    "        loss = criterion(outputs, labels_enc) if LOSS_TYPE == 'bce'\\\n",
    "                                              else criterion(outputs, labels)\n",
    "        loss = criterion(outputs, labels_enc)\n",
    "        cumulative_loss += loss.item()\n",
    "        \n",
    "        if current_step != 0 and current_step % LOG_FREQUENCY == 0:\n",
    "                print('\\t\\tTrain step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        current_step += 1\n",
    "\n",
    "    return cumulative_loss / len(train_loader), running_corrects, current_step\n",
    "\n",
    "def validate(net, val_loader, criterion, optimizer, device=DEVICE):\n",
    "    net.eval()\n",
    "    cumulative_loss =.0\n",
    "    running_corrects = 0\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        if LOSS_TYPE == 'bce':\n",
    "            labels_enc = one_hot_encode_labels(labels).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = net(images)\n",
    "        \n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "        \n",
    "        loss = criterion(outputs, labels_enc) if LOSS_TYPE == 'bce'\\\n",
    "                                              else criterion(outputs, labels)\n",
    "        cumulative_loss += loss.item()\n",
    "\n",
    "\n",
    "    return cumulative_loss / len(val_loader), running_corrects\n",
    "\n",
    "def test(net, test_loader, device=DEVICE):\n",
    "    \n",
    "    running_corrects = 0\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        net.eval()\n",
    "        outputs = net(images)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "\n",
    "    return running_corrects\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "s5SroLpaipNw",
    "colab_type": "text"
   },
   "source": [
    "**FINE TUNING FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "clnGi_eLipNw",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def fine_tuning(train_val_dataset, test_dataset, max_epoch=NUM_EPOCHS, file_path=OUTPUT_PATH, device=DEVICE):\n",
    "    import math, time\n",
    "    incremental_test = []\n",
    "    train_mean_stage_accuracies = []\n",
    "    val_mean_stage_accuracies = []\n",
    "    test_stage_accuracies = []\n",
    "    cudnn.benchmark\n",
    "    net = utils.get_resnet(32).to(device)\n",
    "    criterion = utils.get_criterion(LOSS_TYPE)\n",
    "    start_time = time.time()\n",
    "    for stage in range(10):\n",
    "        optimizer, scheduler = utils.get_otpmizer_scheduler(net.parameters(), LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA)\n",
    "        print(f\"STARTING FINE TUNING STAGE {stage+1}...\")\n",
    "        # Get indices\n",
    "        # 4000 training, 1000 validation\n",
    "        train_idx, val_idx, test_idx = utils.get_kth_batch(train_val_dataset, test_dataset, stage,\n",
    "                                                                 seed=SEED, train_size=.9, get='indices')\n",
    "        \n",
    "        # Make test set incremental\n",
    "        incremental_test.extend(test_idx)\n",
    "        train_set, val_set, test_set = Subset(train_val_dataset, train_idx),\\\n",
    "                                       Subset(train_val_dataset, val_idx),\\\n",
    "                                       Subset(test_dataset, incremental_test)\n",
    "\n",
    "        # Build data loaders\n",
    "        curr_train_loader = utils.get_train_loader(train_set,batch_size=BATCH_SIZE)\n",
    "        curr_val_loader = utils.get_eval_loader(val_set, batch_size=BATCH_SIZE)\n",
    "        curr_test_loader = utils.get_eval_loader(test_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Init results\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        min_val_loss = -1\n",
    "        current_step = 0\n",
    "        tolerance = 10\n",
    "        for epoch in range(max_epoch):\n",
    "            print(f\"\\tSTARTING EPOCH {epoch+1} - LR={scheduler.get_last_lr()}...\")\n",
    "            curr_result = train_batch(net, curr_train_loader, criterion, optimizer, current_step, device)\n",
    "            curr_train_loss = curr_result[0]\n",
    "            curr_train_accuracy = curr_result[1] / float(BATCH_SIZE * len(curr_train_loader))\n",
    "            current_step = curr_result[2]\n",
    "            \n",
    "            train_losses.append(curr_train_loss)\n",
    "            train_accuracies.append(curr_train_accuracy)\n",
    "            scheduler.step()\n",
    "            \n",
    "            curr_val_loss, val_corrects = validate(net, curr_val_loader, criterion, optimizer, device)\n",
    "            val_losses.append(curr_val_loss)\n",
    "            curr_val_accuracy = val_corrects / float(len(val_set))\n",
    "            val_accuracies.append(curr_val_accuracy)\n",
    "            \n",
    "            print(f\"\\t\\tRESULT EPOCH {epoch+1}:\")\n",
    "            print(f\"\\t\\t\\tTrain Loss: {curr_train_loss} - Train Accuracy: {curr_train_accuracy}\")\n",
    "            print(f\"\\t\\t\\tVal Loss: {curr_val_loss} - Val Accuracy: {curr_val_accuracy}\\n\")\n",
    "            \n",
    "            if math.isnan(curr_val_loss):\n",
    "                tolerance -= 1\n",
    "            else:\n",
    "                tolerance = 10\n",
    "            \n",
    "            if tolerance == 0:\n",
    "                print(f\"STAGE {stage+1} -> EARLY STOPPING\\n\")\n",
    "                break\n",
    "            \n",
    "            if min_val_loss == -1 or min_val_loss > curr_val_loss:\n",
    "                min_val_loss = curr_val_loss\n",
    "                torch.save(net, f\"{file_path}_best_model_finetuning.pth\")\n",
    "        \n",
    "        net = torch.load(f\"{file_path}_best_model_finetuning.pth\").to(device)\n",
    "        epoch_test_accuracy = test(net, curr_test_loader, device) / float(len(test_set))\n",
    "        test_stage_accuracies.append(epoch_test_accuracy)\n",
    "        train_mean_stage_accuracies.append(np.mean(train_accuracies))\n",
    "        val_mean_stage_accuracies.append(np.mean(val_accuracies))\n",
    "        \n",
    "        print(f\"\\n\\tResults STAGE {stage+1}:\")\n",
    "        print(f\"\\t\\tTrain Mean Accuracy: {train_mean_stage_accuracies[stage]}\")\n",
    "        print(f\"\\t\\tVal Mean Accuracy: {val_mean_stage_accuracies[stage]}\")\n",
    "        print(f\"\\t\\tTest Accuracy: {test_stage_accuracies[stage]}\\n\")\n",
    "\n",
    "\n",
    "    total_time = int(time.time() - start_time)\n",
    "    min = int(total_time / 60)\n",
    "    sec = total_time % 60\n",
    "    print(f\"\\nTotal time: {min} min {sec} sec\\n\")\n",
    "        \n",
    "    return train_mean_stage_accuracies,\\\n",
    "           val_mean_stage_accuracies,\\\n",
    "           test_stage_accuracies"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sLnI2Dqx6I-",
    "colab_type": "text"
   },
   "source": [
    "**JOINT TRAINING FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "44iaChR_yM34",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def joint_training(train_val_dataset, test_dataset, max_epoch=NUM_EPOCHS, file_path=OUTPUT_PATH, device=DEVICE):\n",
    "    import math, time\n",
    "    incremental_train = []\n",
    "    incremental_val = []\n",
    "    incremental_test = []\n",
    "    train_mean_stage_accuracies = []\n",
    "    val_mean_stage_accuracies = []\n",
    "    test_stage_accuracies = []\n",
    "    cudnn.benchmark\n",
    "    net = utils.get_resnet(32).to(device)\n",
    "    criterion = utils.get_criterion(LOSS_TYPE)\n",
    "    start_time = time.time()\n",
    "    for stage in range(10):\n",
    "        optimizer, scheduler = utils.get_otpmizer_scheduler(net.parameters(), LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA)\n",
    "        print(f\"STARTING JOINT TRAINING STAGE {stage+1}...\")\n",
    "        # Get indices\n",
    "        # 4000 training, 1000 validation\n",
    "        train_idx, val_idx, test_idx = utils.get_kth_batch(train_val_dataset, test_dataset, stage,\n",
    "                                                                 seed=SEED, train_size=.9, get='indices')\n",
    "        \n",
    "        # Make test set incremental\n",
    "        incremental_train.extend(train_idx)\n",
    "        incremental_val.extend(val_idx)\n",
    "        incremental_test.extend(test_idx)\n",
    "        train_set, val_set, test_set = Subset(train_val_dataset, incremental_train),\\\n",
    "                                       Subset(train_val_dataset, incremental_val),\\\n",
    "                                       Subset(test_dataset, incremental_test)\n",
    "\n",
    "\n",
    "        # Build data loaders\n",
    "        curr_train_loader = utils.get_train_loader(train_set,batch_size=BATCH_SIZE)\n",
    "        curr_val_loader = utils.get_eval_loader(val_set, batch_size=BATCH_SIZE)\n",
    "        curr_test_loader = utils.get_eval_loader(test_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Init results\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        min_val_loss = -1\n",
    "        current_step = 0\n",
    "        tolerance = 10\n",
    "        for epoch in range(max_epoch):\n",
    "            print(f\"\\tSTARTING EPOCH {epoch+1} - LR={scheduler.get_last_lr()}...\")\n",
    "            curr_result = train_batch(net, curr_train_loader, criterion, optimizer, current_step, device)\n",
    "            curr_train_loss = curr_result[0]\n",
    "            curr_train_accuracy = curr_result[1] / float(BATCH_SIZE * len(curr_train_loader))\n",
    "            current_step = curr_result[2]\n",
    "            \n",
    "            train_losses.append(curr_train_loss)\n",
    "            train_accuracies.append(curr_train_accuracy)\n",
    "            scheduler.step()\n",
    "            \n",
    "            curr_val_loss, val_corrects = validate(net, curr_val_loader, criterion, optimizer, device)\n",
    "            val_losses.append(curr_val_loss)\n",
    "            curr_val_accuracy = val_corrects / float(len(val_set))\n",
    "            val_accuracies.append(curr_val_accuracy)\n",
    "            \n",
    "            print(f\"\\t\\tRESULT EPOCH {epoch+1}:\")\n",
    "            print(f\"\\t\\t\\tTrain Loss: {curr_train_loss} - Train Accuracy: {curr_train_accuracy}\")\n",
    "            print(f\"\\t\\t\\tVal Loss: {curr_val_loss} - Val Accuracy: {curr_val_accuracy}\\n\")\n",
    "            \n",
    "            if math.isnan(curr_val_loss):\n",
    "                tolerance -= 1\n",
    "            else:\n",
    "                tolerance = 10\n",
    "            \n",
    "            if tolerance == 0:\n",
    "                print(f\"STAGE {stage+1} -> EARLY STOPPING\\n\")\n",
    "                break\n",
    "            \n",
    "            if min_val_loss == -1 or min_val_loss > curr_val_loss:\n",
    "                min_val_loss = curr_val_loss\n",
    "                torch.save(net, f\"{file_path}_best_model_finetuning.pth\")\n",
    "        \n",
    "        net = torch.load(f\"{file_path}_best_model_finetuning.pth\").to(device)\n",
    "        epoch_test_accuracy = test(net, curr_test_loader, device) / float(len(test_set))\n",
    "        test_stage_accuracies.append(epoch_test_accuracy)\n",
    "        train_mean_stage_accuracies.append(np.mean(train_accuracies))\n",
    "        val_mean_stage_accuracies.append(np.mean(val_accuracies))\n",
    "        \n",
    "        print(f\"\\n\\tResults STAGE {stage+1}:\")\n",
    "        print(f\"\\t\\tTrain Mean Accuracy: {train_mean_stage_accuracies[stage]}\")\n",
    "        print(f\"\\t\\tVal Mean Accuracy: {val_mean_stage_accuracies[stage]}\")\n",
    "        print(f\"\\t\\tTest Accuracy: {test_stage_accuracies[stage]}\\n\")\n",
    "\n",
    "\n",
    "    total_time = int(time.time() - start_time)\n",
    "    min = int(total_time / 60)\n",
    "    sec = total_time % 60\n",
    "    print(f\"\\nJOINT TRAININGTotal time: {min} min {sec} sec\\n\")\n",
    "        \n",
    "    return train_mean_stage_accuracies,\\\n",
    "           val_mean_stage_accuracies,\\\n",
    "           test_stage_accuracies"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "bvaYg8SiipNy",
    "colab_type": "text"
   },
   "source": [
    "**FINE TUNING / JOINT TRAINING START**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "i_ejvvl4ipNy",
    "colab_type": "code",
    "outputId": "8ad9c1a3-6305-4e7d-9d5e-e329a81b91ba",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    }
   },
   "source": [
    "train_accuracies,\\\n",
    "val_accuracies,\\\n",
    "test_accuracies = fine_tuning(train_val_dataset, test_dataset, NUM_EPOCHS) if TRAINING_TYPE == 'FT' else joint_training(train_val_dataset, test_dataset, NUM_EPOCHS)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ciGlvEWabcbD",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 643
    },
    "outputId": "bdd55d30-339b-4122-ccd4-d4abcf338ef7"
   },
   "source": [
    "plt.figure()\n",
    "class_batches = np.arange(10, 101, 10)\n",
    "plt.figure()\n",
    "plt.scatter(class_batches, np.array(test_accuracies)*100, zorder = 100, c='orange')\n",
    "plt.plot(class_batches, np.array(test_accuracies)*100)\n",
    "plt.grid()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}