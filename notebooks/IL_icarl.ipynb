{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  },
  "colab": {
   "name": "IL_icarl.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "tvETQMX1ipNf",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1qxQQaeS0EPDO4vItkdJIw6V_cMp5qjMI#scrollTo=5-LoM_h1IXAi\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab Account AI\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5-LoM_h1IXAi",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "a90b822c-948b-4796-b850-679c16edfd5d"
   },
   "source": [
    "# memory footprint support libraries/code\n",
    "\"\"\"!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\"\"\"\n",
    "\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
    "gpu = GPUs[0]\n",
    "print(gpu.name)"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Tesla P100-PCIE-16GB\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "wwN82ZV7ipNg",
    "colab_type": "text"
   },
   "source": [
    "**Import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "RSnex0bmipNh",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "DATASET_ROOT = 'cifar-100-python'\n",
    "CODE_ROOT = 'libs'\n",
    "import os\n",
    "if not os.path.isdir(DATASET_ROOT):\n",
    "    !wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
    "    !tar -xf 'cifar-100-python.tar.gz'  \n",
    "    !rm -rf 'cifar-100-python.tar.gz'\n",
    "\n",
    "if not os.path.isdir(CODE_ROOT):\n",
    "  !git clone https://lore-lml:29f601e814e0446c5b17a9f6c3684d1cbd316bcf@github.com/lore-lml/machine-learning2020-incremental_learning.git\n",
    "  !mv 'machine-learning2020-incremental_learning/libs' '.'\n",
    "  !rm -rf 'machine-learning2020-incremental_learning'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "from torch.backends import cudnn\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import libs.utils as utils\n",
    "from libs.utils import get_one_hot, create_augmented_dataset\n",
    "\n",
    "from libs.models.icarl import iCaRLModel\n",
    "\n",
    "%matplotlib inline"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "7W9y67yoipNk",
    "colab_type": "text"
   },
   "source": [
    "**SET ARGUMENTS**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "r0hjWAP3ipNk",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "\n",
    "\n",
    "arguments = utils.get_arguments()\n",
    "\n",
    "DEVICE = arguments['DEVICE']\n",
    "NUM_CLASSES = arguments[\"NUM_CLASSES\"] \n",
    "\n",
    "BATCH_SIZE = arguments[\"BATCH_SIZE\"]        # Higher batch sizes allows for larger learning rates. An empirical heuristic suggests that, when changing\n",
    "                                            # the batch size, learning rate should change by the same factor to have comparable results\n",
    "\n",
    "LR = arguments[\"LR\"]                        # The initial Learning Rate\n",
    "MOMENTUM = arguments[\"MOMENTUM\"]            # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = arguments[\"WEIGHT_DECAY\"]    # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = arguments[\"NUM_EPOCHS\"]        # Total number of training epochs (iterations over dataset)\n",
    "GAMMA = arguments[\"GAMMA\"]                  # Multiplicative factor for learning rate step-down\n",
    "\n",
    "LOG_FREQUENCY = arguments[\"LOG_FREQUENCY\"]\n",
    "MILESTONES = arguments[\"MILESTONES\"]\n",
    "SEED = arguments[\"SEED\"]\n",
    "\n",
    "CLASSIFIER = \"fc\"\n",
    "HERDING = True\n",
    "\n",
    "OUTPUT_PATH = f\"RUN1_LWF_{CLASSIFIER}\""
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "SaT8eFDNipNm",
    "colab_type": "text"
   },
   "source": [
    "**Define Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "m-ydAGw4ipNm",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "train_transforms, eval_transforms = utils.get_train_eval_transforms()"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "X7Naz_DdipNp",
    "colab_type": "text"
   },
   "source": [
    "**Prepare Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "G-Xct5sNipNp",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "outputId": "6dbf59ba-082c-4340-f160-711a2263424d"
   },
   "source": [
    "train_val_dataset = utils.get_cifar_with_seed(DATASET_ROOT, train_transforms, src='train', seed=SEED)\n",
    "test_dataset = utils.get_cifar_with_seed(DATASET_ROOT, eval_transforms, src='test', seed=SEED)\n",
    "\n",
    "print(f\"Size Training Set: {len(train_val_dataset)}\")\n",
    "print(f\"Size Test Set: {len(test_dataset)}\")"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Size Training Set: 50000\n",
      "Size Test Set: 10000\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "xZDP5yXBipNt",
    "colab_type": "text"
   },
   "source": [
    "**Train, Test, Validation functions**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "secPALBtipNt",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def train_batch(net: iCaRLModel, train_loader, optimizer, current_step, device=DEVICE):\n",
    "    net.train()\n",
    "    cumulative_loss =.0\n",
    "    running_corrects = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(images)\n",
    "        \n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "        \n",
    "        loss = net.compute_distillation_loss(images, labels, outputs, DEVICE)\n",
    "        cumulative_loss += loss.item()\n",
    "        \n",
    "        if current_step != 0 and current_step % LOG_FREQUENCY == 0:\n",
    "                print('\\t\\tTrain step - Step {}, Loss {}'.format(current_step, loss.item()))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        current_step += 1\n",
    "\n",
    "    return cumulative_loss / len(train_loader), running_corrects, current_step\n",
    "\n",
    "def test(net: iCaRLModel, test_loader, device=DEVICE):\n",
    "    print(\"TEEEEEEEEEEEEST\")\n",
    "    # confusion matrix\n",
    "    y_true = []\n",
    "    y_preds = []\n",
    "\n",
    "    running_corrects = 0\n",
    "    net.eval()\n",
    "    for images, labels in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = net.classify(images, CLASSIFIER).to(device)\n",
    "        _, preds = torch.max(outputs.data, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data).data.item()\n",
    "\n",
    "        # confusion matrix\n",
    "        y_true.extend(labels.data.tolist())\n",
    "        y_preds.extend(preds.tolist())\n",
    "\n",
    "   \n",
    "    return running_corrects, y_true, y_preds\n"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "s5SroLpaipNw",
    "colab_type": "text"
   },
   "source": [
    "**iCaRL FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "clnGi_eLipNw",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def icarl_training(train_val_dataset, test_dataset, max_epoch=NUM_EPOCHS, file_path=OUTPUT_PATH, device=DEVICE):\n",
    "    import math, time\n",
    "    incremental_test = []\n",
    "    train_mean_stage_accuracies = []\n",
    "    test_stage_accuracies = []\n",
    "    \n",
    "    cudnn.benchmark\n",
    "    net = iCaRLModel(num_classes=100)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for stage in range(10):\n",
    "        optimizer, scheduler = utils.get_otpmizer_scheduler(net.parameters(), LR, MOMENTUM, WEIGHT_DECAY, MILESTONES, GAMMA)\n",
    "        print(f\"STARTING FINE TUNING STAGE {stage+1}...\")\n",
    "        # Get indices\n",
    "        # 4000 training, 1000 validation\n",
    "        train_idx, test_idx = utils.get_idxs_per_class_of_kth_batch(train_val_dataset, test_dataset, stage)\n",
    "        \n",
    "        # Make test set incremental\n",
    "        incremental_test.extend(np.ravel(test_idx))\n",
    "        subsets_per_class = [Subset(train_val_dataset, idx_per_class) for idx_per_class in train_idx]\n",
    "        train_idx = np.ravel(train_idx)\n",
    "        train_set, test_set = Subset(train_val_dataset, train_idx), Subset(test_dataset, incremental_test)\n",
    "        \n",
    "        exemplars_idx = net.before_train(DEVICE)\n",
    "        print(exemplars_idx)\n",
    "        print(len(exemplars_idx))\n",
    "        if len(exemplars_idx) > 0:\n",
    "            exemplars_subset = Subset(train_val_dataset, exemplars_idx)\n",
    "            train_set = create_augmented_dataset(train_set, exemplars_subset)\n",
    "        \n",
    "        # Build data loaders\n",
    "        curr_train_loader = utils.get_train_loader(train_set,batch_size=BATCH_SIZE)\n",
    "        curr_test_loader = utils.get_eval_loader(test_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # Init results\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        current_step = 0\n",
    "        tolerance = 10\n",
    "        for epoch in range(max_epoch):\n",
    "            print(f\"\\tSTARTING EPOCH {epoch+1} - LR={scheduler.get_last_lr()}...\")\n",
    "            curr_result = train_batch(net, curr_train_loader, optimizer, current_step, device)\n",
    "            curr_train_loss = curr_result[0]\n",
    "            curr_train_accuracy = curr_result[1] / float(BATCH_SIZE * len(curr_train_loader))\n",
    "            current_step = curr_result[2]\n",
    "            \n",
    "            train_losses.append(curr_train_loss)\n",
    "            train_accuracies.append(curr_train_accuracy)\n",
    "            scheduler.step()\n",
    "            \n",
    "            print(f\"\\t\\tRESULT EPOCH {epoch+1}:\")\n",
    "            print(f\"\\t\\t\\tTrain Loss: {curr_train_loss} - Train Accuracy: {curr_train_accuracy}\\n\")\n",
    "            \n",
    "            if math.isnan(curr_train_loss):\n",
    "                tolerance -= 1\n",
    "            else:\n",
    "                tolerance = 10\n",
    "            \n",
    "            if tolerance == 0:\n",
    "                print(f\"STAGE {stage+1} -> EARLY STOPPING\\n\")\n",
    "                break\n",
    "        \n",
    "        net.after_train(10, subsets_per_class, np.arange(10*stage, 10*(stage+1)), DEVICE, herding=HERDING)\n",
    "        corrects, y_true, y_preds = test(net, curr_test_loader, device)\n",
    "        epoch_test_accuracy = corrects / float(len(test_set))\n",
    "        test_stage_accuracies.append(epoch_test_accuracy)\n",
    "        train_mean_stage_accuracies.append(np.mean(train_accuracies))\n",
    "        \n",
    "        print(f\"\\n\\tResults STAGE {stage+1}:\")\n",
    "        print(f\"\\t\\tTrain Mean Accuracy: {train_mean_stage_accuracies[stage]}\")\n",
    "        print(f\"\\t\\tTest Accuracy: {test_stage_accuracies[stage]}\\n\")\n",
    "\n",
    "\n",
    "    total_time = int(time.time() - start_time)\n",
    "    min = int(total_time / 60)\n",
    "    sec = total_time % 60\n",
    "    print(f\"\\nTotal time: {min} min {sec} sec\\n\")\n",
    "    \n",
    "    return train_mean_stage_accuracies,\\\n",
    "           test_stage_accuracies,\\\n",
    "           y_true, y_preds"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "bvaYg8SiipNy",
    "colab_type": "text"
   },
   "source": [
    "**LEARNING WITHOUT FORGETTING START**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "i_ejvvl4ipNy",
    "colab_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "60b44ac5-a274-4966-e232-6a02038462ed"
   },
   "source": [
    "train_accuracies,\\\n",
    "test_accuracies,\\\n",
    "y_true, y_preds = icarl_training(train_val_dataset, test_dataset, NUM_EPOCHS)"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "STARTING FINE TUNING STAGE 1...\n",
      "[]\n",
      "0\n",
      "\tSTARTING EPOCH 1 - LR=[2]...\n",
      "\t\tTrain step - Step 30, Loss 0.02996712550520897\n",
      "\t\tRESULT EPOCH 1:\n",
      "\t\t\tTrain Loss: 0.06438378130014126 - Train Accuracy: 0.16947115384615385\n",
      "\n",
      "\tSTARTING EPOCH 2 - LR=[2]...\n",
      "\t\tTrain step - Step 60, Loss 0.026844093576073647\n",
      "\t\tRESULT EPOCH 2:\n",
      "\t\t\tTrain Loss: 0.026381173481543858 - Train Accuracy: 0.3762019230769231\n",
      "\n",
      "\tSTARTING EPOCH 3 - LR=[2]...\n",
      "\t\tTrain step - Step 90, Loss 0.023733116686344147\n",
      "\t\tRESULT EPOCH 3:\n",
      "\t\t\tTrain Loss: 0.023641024883358907 - Train Accuracy: 0.4519230769230769\n",
      "\n",
      "\tSTARTING EPOCH 4 - LR=[2]...\n",
      "\t\tTrain step - Step 120, Loss 0.022702902555465698\n",
      "\t\tTrain step - Step 150, Loss 0.01845630444586277\n",
      "\t\tRESULT EPOCH 4:\n",
      "\t\t\tTrain Loss: 0.02144761584126032 - Train Accuracy: 0.5060096153846154\n",
      "\n",
      "\tSTARTING EPOCH 5 - LR=[2]...\n",
      "\t\tTrain step - Step 180, Loss 0.01796424761414528\n",
      "\t\tRESULT EPOCH 5:\n",
      "\t\t\tTrain Loss: 0.019986193865919724 - Train Accuracy: 0.5414663461538461\n",
      "\n",
      "\tSTARTING EPOCH 6 - LR=[2]...\n",
      "\t\tTrain step - Step 210, Loss 0.02020999789237976\n",
      "\t\tRESULT EPOCH 6:\n",
      "\t\t\tTrain Loss: 0.01892313509224317 - Train Accuracy: 0.5807291666666666\n",
      "\n",
      "\tSTARTING EPOCH 7 - LR=[2]...\n",
      "\t\tTrain step - Step 240, Loss 0.017538677901029587\n",
      "\t\tTrain step - Step 270, Loss 0.020129410549998283\n",
      "\t\tRESULT EPOCH 7:\n",
      "\t\t\tTrain Loss: 0.017664686370736513 - Train Accuracy: 0.609375\n",
      "\n",
      "\tSTARTING EPOCH 8 - LR=[2]...\n",
      "\t\tTrain step - Step 300, Loss 0.01583426259458065\n",
      "\t\tRESULT EPOCH 8:\n",
      "\t\t\tTrain Loss: 0.016888758955666654 - Train Accuracy: 0.639823717948718\n",
      "\n",
      "\tSTARTING EPOCH 9 - LR=[2]...\n",
      "\t\tTrain step - Step 330, Loss 0.01619521901011467\n",
      "\t\tRESULT EPOCH 9:\n",
      "\t\t\tTrain Loss: 0.016179306743045647 - Train Accuracy: 0.6532451923076923\n",
      "\n",
      "\tSTARTING EPOCH 10 - LR=[2]...\n",
      "\t\tTrain step - Step 360, Loss 0.01689237356185913\n",
      "\t\tRESULT EPOCH 10:\n",
      "\t\t\tTrain Loss: 0.015590169299871493 - Train Accuracy: 0.6636618589743589\n",
      "\n",
      "\tSTARTING EPOCH 11 - LR=[2]...\n",
      "\t\tTrain step - Step 390, Loss 0.014197871088981628\n",
      "\t\tTrain step - Step 420, Loss 0.014310305006802082\n",
      "\t\tRESULT EPOCH 11:\n",
      "\t\t\tTrain Loss: 0.014405375824142724 - Train Accuracy: 0.6957131410256411\n",
      "\n",
      "\tSTARTING EPOCH 12 - LR=[2]...\n",
      "\t\tTrain step - Step 450, Loss 0.016527585685253143\n",
      "\t\tRESULT EPOCH 12:\n",
      "\t\t\tTrain Loss: 0.01398473424025071 - Train Accuracy: 0.7135416666666666\n",
      "\n",
      "\tSTARTING EPOCH 13 - LR=[2]...\n",
      "\t\tTrain step - Step 480, Loss 0.010885069146752357\n",
      "\t\tRESULT EPOCH 13:\n",
      "\t\t\tTrain Loss: 0.013171953316300344 - Train Accuracy: 0.7253605769230769\n",
      "\n",
      "\tSTARTING EPOCH 14 - LR=[2]...\n",
      "\t\tTrain step - Step 510, Loss 0.014551417902112007\n",
      "\t\tTrain step - Step 540, Loss 0.015243944711983204\n",
      "\t\tRESULT EPOCH 14:\n",
      "\t\t\tTrain Loss: 0.012263169488272605 - Train Accuracy: 0.7534054487179487\n",
      "\n",
      "\tSTARTING EPOCH 15 - LR=[2]...\n",
      "\t\tTrain step - Step 570, Loss 0.01168231200426817\n",
      "\t\tRESULT EPOCH 15:\n",
      "\t\t\tTrain Loss: 0.01241981302602933 - Train Accuracy: 0.7477964743589743\n",
      "\n",
      "\tSTARTING EPOCH 16 - LR=[2]...\n",
      "\t\tTrain step - Step 600, Loss 0.014415823854506016\n",
      "\t\tRESULT EPOCH 16:\n",
      "\t\t\tTrain Loss: 0.011905060293009648 - Train Accuracy: 0.7606169871794872\n",
      "\n",
      "\tSTARTING EPOCH 17 - LR=[2]...\n",
      "\t\tTrain step - Step 630, Loss 0.010352249257266521\n",
      "\t\tTrain step - Step 660, Loss 0.011019992642104626\n",
      "\t\tRESULT EPOCH 17:\n",
      "\t\t\tTrain Loss: 0.011126713540691596 - Train Accuracy: 0.7754407051282052\n",
      "\n",
      "\tSTARTING EPOCH 18 - LR=[2]...\n",
      "\t\tTrain step - Step 690, Loss 0.009771334938704967\n",
      "\t\tRESULT EPOCH 18:\n",
      "\t\t\tTrain Loss: 0.010717286155200921 - Train Accuracy: 0.7878605769230769\n",
      "\n",
      "\tSTARTING EPOCH 19 - LR=[2]...\n",
      "\t\tTrain step - Step 720, Loss 0.010906464420258999\n",
      "\t\tRESULT EPOCH 19:\n",
      "\t\t\tTrain Loss: 0.010454469909652686 - Train Accuracy: 0.7890625\n",
      "\n",
      "\tSTARTING EPOCH 20 - LR=[2]...\n",
      "\t\tTrain step - Step 750, Loss 0.009470956400036812\n",
      "\t\tRESULT EPOCH 20:\n",
      "\t\t\tTrain Loss: 0.009829709031738533 - Train Accuracy: 0.8050881410256411\n",
      "\n",
      "\tSTARTING EPOCH 21 - LR=[2]...\n",
      "\t\tTrain step - Step 780, Loss 0.008072505705058575\n",
      "\t\tTrain step - Step 810, Loss 0.011745979078114033\n",
      "\t\tRESULT EPOCH 21:\n",
      "\t\t\tTrain Loss: 0.009311203235903611 - Train Accuracy: 0.8114983974358975\n",
      "\n",
      "\tSTARTING EPOCH 22 - LR=[2]...\n",
      "\t\tTrain step - Step 840, Loss 0.009227624163031578\n",
      "\t\tRESULT EPOCH 22:\n",
      "\t\t\tTrain Loss: 0.009052556628982225 - Train Accuracy: 0.8225160256410257\n",
      "\n",
      "\tSTARTING EPOCH 23 - LR=[2]...\n",
      "\t\tTrain step - Step 870, Loss 0.007942115887999535\n",
      "\t\tRESULT EPOCH 23:\n",
      "\t\t\tTrain Loss: 0.008876965548365544 - Train Accuracy: 0.8219150641025641\n",
      "\n",
      "\tSTARTING EPOCH 24 - LR=[2]...\n",
      "\t\tTrain step - Step 900, Loss 0.0065201204270124435\n",
      "\t\tTrain step - Step 930, Loss 0.008936364203691483\n",
      "\t\tRESULT EPOCH 24:\n",
      "\t\t\tTrain Loss: 0.0085166141581841 - Train Accuracy: 0.8293269230769231\n",
      "\n",
      "\tSTARTING EPOCH 25 - LR=[2]...\n",
      "\t\tTrain step - Step 960, Loss 0.007311967667192221\n",
      "\t\tRESULT EPOCH 25:\n",
      "\t\t\tTrain Loss: 0.008436580212452473 - Train Accuracy: 0.8325320512820513\n",
      "\n",
      "\tSTARTING EPOCH 26 - LR=[2]...\n",
      "\t\tTrain step - Step 990, Loss 0.008216491900384426\n",
      "\t\tRESULT EPOCH 26:\n",
      "\t\t\tTrain Loss: 0.007626213360195741 - Train Accuracy: 0.8531650641025641\n",
      "\n",
      "\tSTARTING EPOCH 27 - LR=[2]...\n",
      "\t\tTrain step - Step 1020, Loss 0.008666197769343853\n",
      "\t\tTrain step - Step 1050, Loss 0.008426978252828121\n",
      "\t\tRESULT EPOCH 27:\n",
      "\t\t\tTrain Loss: 0.007689156342679873 - Train Accuracy: 0.852363782051282\n",
      "\n",
      "\tSTARTING EPOCH 28 - LR=[2]...\n",
      "\t\tTrain step - Step 1080, Loss 0.008769967593252659\n",
      "\t\tRESULT EPOCH 28:\n",
      "\t\t\tTrain Loss: 0.007343936186188307 - Train Accuracy: 0.8589743589743589\n",
      "\n",
      "\tSTARTING EPOCH 29 - LR=[2]...\n",
      "\t\tTrain step - Step 1110, Loss 0.0044714235700666904\n",
      "\t\tRESULT EPOCH 29:\n",
      "\t\t\tTrain Loss: 0.006769281812012196 - Train Accuracy: 0.8701923076923077\n",
      "\n",
      "\tSTARTING EPOCH 30 - LR=[2]...\n",
      "\t\tTrain step - Step 1140, Loss 0.005707123316824436\n",
      "\t\tRESULT EPOCH 30:\n",
      "\t\t\tTrain Loss: 0.007146405008358833 - Train Accuracy: 0.8673878205128205\n",
      "\n",
      "\tSTARTING EPOCH 31 - LR=[2]...\n",
      "\t\tTrain step - Step 1170, Loss 0.00812976248562336\n",
      "\t\tTrain step - Step 1200, Loss 0.0066290805116295815\n",
      "\t\tRESULT EPOCH 31:\n",
      "\t\t\tTrain Loss: 0.00677426872201837 - Train Accuracy: 0.867988782051282\n",
      "\n",
      "\tSTARTING EPOCH 32 - LR=[2]...\n",
      "\t\tTrain step - Step 1230, Loss 0.005277461837977171\n",
      "\t\tRESULT EPOCH 32:\n",
      "\t\t\tTrain Loss: 0.006858546442041795 - Train Accuracy: 0.8731971153846154\n",
      "\n",
      "\tSTARTING EPOCH 33 - LR=[2]...\n",
      "\t\tTrain step - Step 1260, Loss 0.007398190908133984\n",
      "\t\tRESULT EPOCH 33:\n",
      "\t\t\tTrain Loss: 0.006363937058127844 - Train Accuracy: 0.8814102564102564\n",
      "\n",
      "\tSTARTING EPOCH 34 - LR=[2]...\n",
      "\t\tTrain step - Step 1290, Loss 0.004551445599645376\n",
      "\t\tTrain step - Step 1320, Loss 0.006461541634052992\n",
      "\t\tRESULT EPOCH 34:\n",
      "\t\t\tTrain Loss: 0.0060385630872005075 - Train Accuracy: 0.8854166666666666\n",
      "\n",
      "\tSTARTING EPOCH 35 - LR=[2]...\n",
      "\t\tTrain step - Step 1350, Loss 0.004741136450320482\n",
      "\t\tRESULT EPOCH 35:\n",
      "\t\t\tTrain Loss: 0.005782244112103796 - Train Accuracy: 0.8896233974358975\n",
      "\n",
      "\tSTARTING EPOCH 36 - LR=[2]...\n",
      "\t\tTrain step - Step 1380, Loss 0.004578664433211088\n",
      "\t\tRESULT EPOCH 36:\n",
      "\t\t\tTrain Loss: 0.005685874404242406 - Train Accuracy: 0.8954326923076923\n",
      "\n",
      "\tSTARTING EPOCH 37 - LR=[2]...\n",
      "\t\tTrain step - Step 1410, Loss 0.005089181009680033\n",
      "\t\tTrain step - Step 1440, Loss 0.0055020712316036224\n",
      "\t\tRESULT EPOCH 37:\n",
      "\t\t\tTrain Loss: 0.00573363668547991 - Train Accuracy: 0.8928285256410257\n",
      "\n",
      "\tSTARTING EPOCH 38 - LR=[2]...\n",
      "\t\tTrain step - Step 1470, Loss 0.006746755912899971\n",
      "\t\tRESULT EPOCH 38:\n",
      "\t\t\tTrain Loss: 0.005207561147518647 - Train Accuracy: 0.9072516025641025\n",
      "\n",
      "\tSTARTING EPOCH 39 - LR=[2]...\n",
      "\t\tTrain step - Step 1500, Loss 0.005309860687702894\n",
      "\t\tRESULT EPOCH 39:\n",
      "\t\t\tTrain Loss: 0.005198284726924239 - Train Accuracy: 0.9050480769230769\n",
      "\n",
      "\tSTARTING EPOCH 40 - LR=[2]...\n",
      "\t\tTrain step - Step 1530, Loss 0.005642405711114407\n",
      "\t\tRESULT EPOCH 40:\n",
      "\t\t\tTrain Loss: 0.004887125759313886 - Train Accuracy: 0.9136618589743589\n",
      "\n",
      "\tSTARTING EPOCH 41 - LR=[2]...\n",
      "\t\tTrain step - Step 1560, Loss 0.004345003515481949\n",
      "\t\tTrain step - Step 1590, Loss 0.004148727748543024\n",
      "\t\tRESULT EPOCH 41:\n",
      "\t\t\tTrain Loss: 0.004758546785570872 - Train Accuracy: 0.9104567307692307\n",
      "\n",
      "\tSTARTING EPOCH 42 - LR=[2]...\n",
      "\t\tTrain step - Step 1620, Loss 0.004888075403869152\n",
      "\t\tRESULT EPOCH 42:\n",
      "\t\t\tTrain Loss: 0.0048163710400844235 - Train Accuracy: 0.9120592948717948\n",
      "\n",
      "\tSTARTING EPOCH 43 - LR=[2]...\n",
      "\t\tTrain step - Step 1650, Loss 0.006127841770648956\n",
      "\t\tRESULT EPOCH 43:\n",
      "\t\t\tTrain Loss: 0.005205785467599829 - Train Accuracy: 0.9036458333333334\n",
      "\n",
      "\tSTARTING EPOCH 44 - LR=[2]...\n",
      "\t\tTrain step - Step 1680, Loss 0.005498190876096487\n",
      "\t\tTrain step - Step 1710, Loss 0.0037488865200430155\n",
      "\t\tRESULT EPOCH 44:\n",
      "\t\t\tTrain Loss: 0.00432603191345548 - Train Accuracy: 0.921875\n",
      "\n",
      "\tSTARTING EPOCH 45 - LR=[2]...\n",
      "\t\tTrain step - Step 1740, Loss 0.003988099750131369\n",
      "\t\tRESULT EPOCH 45:\n",
      "\t\t\tTrain Loss: 0.004289800289254158 - Train Accuracy: 0.9214743589743589\n",
      "\n",
      "\tSTARTING EPOCH 46 - LR=[2]...\n",
      "\t\tTrain step - Step 1770, Loss 0.004255293402820826\n",
      "\t\tRESULT EPOCH 46:\n",
      "\t\t\tTrain Loss: 0.004665522471977732 - Train Accuracy: 0.9142628205128205\n",
      "\n",
      "\tSTARTING EPOCH 47 - LR=[2]...\n",
      "\t\tTrain step - Step 1800, Loss 0.004320587031543255\n",
      "\t\tTrain step - Step 1830, Loss 0.0038840274792164564\n",
      "\t\tRESULT EPOCH 47:\n",
      "\t\t\tTrain Loss: 0.004202301160265238 - Train Accuracy: 0.9264823717948718\n",
      "\n",
      "\tSTARTING EPOCH 48 - LR=[2]...\n",
      "\t\tTrain step - Step 1860, Loss 0.004367375746369362\n",
      "\t\tRESULT EPOCH 48:\n",
      "\t\t\tTrain Loss: 0.004048810794185369 - Train Accuracy: 0.9262820512820513\n",
      "\n",
      "\tSTARTING EPOCH 49 - LR=[2]...\n",
      "\t\tTrain step - Step 1890, Loss 0.0030124234035611153\n",
      "\t\tRESULT EPOCH 49:\n",
      "\t\t\tTrain Loss: 0.0036948552976051965 - Train Accuracy: 0.9330929487179487\n",
      "\n",
      "\tSTARTING EPOCH 50 - LR=[0.4]...\n",
      "\t\tTrain step - Step 1920, Loss 0.0023683069739490747\n",
      "\t\tRESULT EPOCH 50:\n",
      "\t\t\tTrain Loss: 0.002628232767351736 - Train Accuracy: 0.9605368589743589\n",
      "\n",
      "\tSTARTING EPOCH 51 - LR=[0.4]...\n",
      "\t\tTrain step - Step 1950, Loss 0.0013092438457533717\n",
      "\t\tTrain step - Step 1980, Loss 0.002705614548176527\n",
      "\t\tRESULT EPOCH 51:\n",
      "\t\t\tTrain Loss: 0.0018971159439295148 - Train Accuracy: 0.9717548076923077\n",
      "\n",
      "\tSTARTING EPOCH 52 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2010, Loss 0.0016168308211490512\n",
      "\t\tRESULT EPOCH 52:\n",
      "\t\t\tTrain Loss: 0.0015879262011880295 - Train Accuracy: 0.9771634615384616\n",
      "\n",
      "\tSTARTING EPOCH 53 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2040, Loss 0.0008740213233977556\n",
      "\t\tRESULT EPOCH 53:\n",
      "\t\t\tTrain Loss: 0.0015698100741689976 - Train Accuracy: 0.9771634615384616\n",
      "\n",
      "\tSTARTING EPOCH 54 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2070, Loss 0.002085685031488538\n",
      "\t\tTrain step - Step 2100, Loss 0.0014303902862593532\n",
      "\t\tRESULT EPOCH 54:\n",
      "\t\t\tTrain Loss: 0.0014771908306731628 - Train Accuracy: 0.9801682692307693\n",
      "\n",
      "\tSTARTING EPOCH 55 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2130, Loss 0.0018861358985304832\n",
      "\t\tRESULT EPOCH 55:\n",
      "\t\t\tTrain Loss: 0.0013205450869953404 - Train Accuracy: 0.9845753205128205\n",
      "\n",
      "\tSTARTING EPOCH 56 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2160, Loss 0.0008857384673319757\n",
      "\t\tRESULT EPOCH 56:\n",
      "\t\t\tTrain Loss: 0.0013152022580377376 - Train Accuracy: 0.984375\n",
      "\n",
      "\tSTARTING EPOCH 57 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2190, Loss 0.0012060028966516256\n",
      "\t\tTrain step - Step 2220, Loss 0.0007327388739213347\n",
      "\t\tRESULT EPOCH 57:\n",
      "\t\t\tTrain Loss: 0.0012324041857097584 - Train Accuracy: 0.9847756410256411\n",
      "\n",
      "\tSTARTING EPOCH 58 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2250, Loss 0.000926386856008321\n",
      "\t\tRESULT EPOCH 58:\n",
      "\t\t\tTrain Loss: 0.0011803640951163685 - Train Accuracy: 0.9855769230769231\n",
      "\n",
      "\tSTARTING EPOCH 59 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2280, Loss 0.0009178743930533528\n",
      "\t\tRESULT EPOCH 59:\n",
      "\t\t\tTrain Loss: 0.0011607680121997897 - Train Accuracy: 0.9881810897435898\n",
      "\n",
      "\tSTARTING EPOCH 60 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2310, Loss 0.0010386393405497074\n",
      "\t\tRESULT EPOCH 60:\n",
      "\t\t\tTrain Loss: 0.0010600032774397195 - Train Accuracy: 0.9885817307692307\n",
      "\n",
      "\tSTARTING EPOCH 61 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2340, Loss 0.000678031356073916\n",
      "\t\tTrain step - Step 2370, Loss 0.0011482591507956386\n",
      "\t\tRESULT EPOCH 61:\n",
      "\t\t\tTrain Loss: 0.0009714034721792603 - Train Accuracy: 0.9875801282051282\n",
      "\n",
      "\tSTARTING EPOCH 62 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2400, Loss 0.001571174245327711\n",
      "\t\tRESULT EPOCH 62:\n",
      "\t\t\tTrain Loss: 0.0010291875119153888 - Train Accuracy: 0.9879807692307693\n",
      "\n",
      "\tSTARTING EPOCH 63 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2430, Loss 0.0003916877321898937\n",
      "\t\tRESULT EPOCH 63:\n",
      "\t\t\tTrain Loss: 0.000984046608955862 - Train Accuracy: 0.9873798076923077\n",
      "\n",
      "\tSTARTING EPOCH 64 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 2460, Loss 0.00037278211675584316\n",
      "\t\tTrain step - Step 2490, Loss 0.00043992235441692173\n",
      "\t\tRESULT EPOCH 64:\n",
      "\t\t\tTrain Loss: 0.0008981595269571512 - Train Accuracy: 0.9895833333333334\n",
      "\n",
      "\tSTARTING EPOCH 65 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 2520, Loss 0.0007888698601163924\n",
      "\t\tRESULT EPOCH 65:\n",
      "\t\t\tTrain Loss: 0.0007888676116589266 - Train Accuracy: 0.991386217948718\n",
      "\n",
      "\tSTARTING EPOCH 66 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 2550, Loss 0.0013900473713874817\n",
      "\t\tRESULT EPOCH 66:\n",
      "\t\t\tTrain Loss: 0.0008189207299624402 - Train Accuracy: 0.9917868589743589\n",
      "\n",
      "\tSTARTING EPOCH 67 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 2580, Loss 0.0005945554003119469\n",
      "\t\tTrain step - Step 2610, Loss 0.0008963783038780093\n",
      "\t\tRESULT EPOCH 67:\n",
      "\t\t\tTrain Loss: 0.0007408019386602041 - Train Accuracy: 0.9935897435897436\n",
      "\n",
      "\tSTARTING EPOCH 68 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 2640, Loss 0.0011863431427627802\n",
      "\t\tRESULT EPOCH 68:\n",
      "\t\t\tTrain Loss: 0.0007429571959596032 - Train Accuracy: 0.992988782051282\n",
      "\n",
      "\tSTARTING EPOCH 69 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 2670, Loss 0.0007965419208630919\n",
      "\t\tRESULT EPOCH 69:\n",
      "\t\t\tTrain Loss: 0.0007338730098966223 - Train Accuracy: 0.9925881410256411\n",
      "\n",
      "\tSTARTING EPOCH 70 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 2700, Loss 0.0006288666627369821\n",
      "\t\tRESULT EPOCH 70:\n",
      "\t\t\tTrain Loss: 0.0007281491849332666 - Train Accuracy: 0.9935897435897436\n",
      "\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/8 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "TEEEEEEEEEEEEST\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 13.57it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\tResults STAGE 1:\n",
      "\t\tTrain Mean Accuracy: 0.8434065934065934\n",
      "\t\tTest Accuracy: 0.857\n",
      "\n",
      "STARTING FINE TUNING STAGE 2...\n",
      "[37732, 509, 509, 2020, 57, 9587, 118, 14710, 8023, 2020, 14710, 14710, 57, 57, 57, 57, 509, 14710, 57, 14710, 57, 9587, 57, 9587, 509, 509, 57, 10234, 57, 9587, 57, 57, 9587, 9587, 57, 9587, 10234, 57, 509, 57, 57, 9587, 9587, 57, 4623, 57, 4623, 9587, 9587, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 33643, 3709, 449, 449, 449, 449, 449, 985, 985, 3636, 48697, 449, 449, 3620, 449, 3620, 3620, 3636, 3620, 3620, 30006, 449, 313, 313, 36355, 372, 313, 313, 449, 36355, 36355, 36355, 449, 313, 313, 313, 1532, 313, 313, 313, 1532, 36355, 313, 1532, 313, 313, 36355, 313, 313, 1532, 36355, 313, 313, 313, 313, 313, 313, 36355, 313, 313, 1532, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 449, 449, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 449, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 449, 313, 313, 313, 313, 313, 313, 313, 313, 313, 449, 313, 313, 313, 313, 313, 43149, 8949, 48806, 8949, 911, 48806, 120, 120, 13416, 120, 120, 439, 120, 120, 120, 120, 1292, 120, 13416, 1292, 120, 120, 13416, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 1292, 120, 120, 120, 1292, 1292, 120, 1292, 120, 1292, 1292, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 1292, 120, 1292, 1292, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 1292, 120, 120, 120, 120, 120, 120, 120, 439, 120, 439, 120, 439, 120, 120, 439, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 439, 1292, 120, 1292, 120, 120, 120, 120, 120, 120, 120, 13416, 120, 120, 120, 1292, 120, 120, 1292, 120, 1292, 120, 120, 120, 120, 120, 120, 120, 27295, 2057, 6943, 1726, 27, 27, 27, 27, 27, 27, 7172, 1726, 1726, 1726, 14145, 27, 27, 27, 27, 27, 27, 27, 27, 27, 8302, 27, 27, 27, 27, 4661, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 4661, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 847, 2057, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 24723, 2852, 5375, 219, 219, 31381, 219, 9101, 31381, 16, 575, 575, 16, 16, 16, 16, 16, 4221, 5375, 16, 7101, 16, 16, 16, 219, 16, 16, 219, 5375, 1255, 16, 16, 16, 16, 7101, 16, 16, 16, 16, 219, 219, 219, 219, 16, 219, 219, 16, 16, 219, 16, 16, 219, 16, 219, 16, 16, 219, 31381, 16, 16, 16, 16, 219, 219, 219, 16, 219, 219, 219, 4448, 219, 4448, 16, 16, 16, 4448, 219, 4448, 16, 30, 16, 16, 4448, 219, 4448, 4448, 4448, 219, 4448, 4448, 30, 219, 30, 16, 4448, 4448, 16, 219, 16, 16, 219, 16, 219, 219, 4448, 16, 16, 219, 16, 16, 16, 16, 219, 4448, 4448, 219, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 4448, 16, 16, 16, 16, 16, 16, 16, 219, 16, 16, 219, 16, 16, 219, 16, 4448, 16, 16, 16, 16, 16, 16, 16, 4448, 4448, 16, 16, 16, 4448, 219, 30, 30, 30, 30, 5375, 5375, 16, 219, 5375, 30, 219, 219, 5375, 5375, 16, 5375, 5375, 30, 16, 16, 219, 30, 30, 16, 30, 30, 30, 5375, 16, 5375, 16, 16, 30, 30, 30, 30, 5375, 29770, 25520, 148, 12106, 23617, 148, 148, 148, 148, 36079, 148, 20, 536, 148, 536, 148, 148, 148, 148, 148, 148, 148, 665, 25520, 148, 665, 665, 148, 148, 25520, 665, 20, 25520, 665, 20, 20, 20, 20, 20, 20, 25520, 665, 665, 20, 20, 25520, 148, 20, 148, 20, 20, 20, 20, 20, 20, 148, 148, 20, 148, 20, 148, 20, 20, 20, 20, 20, 20, 25520, 20, 20, 148, 25520, 20, 20, 148, 148, 148, 25520, 20, 20, 20, 20, 20, 20, 20, 20, 20, 148, 20, 20, 20, 25520, 20, 20, 20, 20, 20, 20, 25520, 20, 20, 20, 25520, 20, 20, 20, 20, 20, 20, 20, 148, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 148, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 148, 20, 20, 148, 20, 20, 20, 148, 20, 20, 20, 148, 20, 20, 20, 20, 20, 20, 20, 20, 20, 20, 13806, 1558, 6938, 1558, 6938, 1558, 6938, 172, 1558, 24295, 43986, 34, 172, 172, 34, 172, 34, 172, 34, 24295, 1558, 172, 1558, 34, 172, 25397, 25397, 25397, 34, 25397, 25397, 34, 34, 34, 34, 172, 172, 172, 172, 172, 34, 172, 172, 172, 172, 34, 34, 34, 34, 34, 172, 34, 34, 172, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 172, 34, 172, 172, 34, 34, 34, 34, 172, 172, 34, 34, 34, 34, 172, 34, 172, 172, 34, 34, 34, 172, 34, 172, 172, 34, 34, 172, 34, 34, 34, 34, 172, 34, 172, 34, 34, 34, 34, 34, 34, 172, 34, 34, 34, 172, 172, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 172, 172, 34, 34, 172, 34, 34, 34, 34, 172, 34, 172, 172, 34, 34, 34, 172, 172, 34, 34, 34, 34, 172, 34, 172, 34, 172, 34, 172, 34, 172, 34, 34, 34, 34, 172, 34, 34, 172, 172, 34, 34, 34, 34, 34, 172, 34, 172, 172, 172, 34, 34, 34, 34, 172, 34, 34, 34, 34, 34, 44516, 7163, 4056, 124, 4614, 1789, 4056, 2774, 4056, 4056, 14205, 14205, 26774, 8287, 43171, 4056, 124, 124, 124, 124, 124, 124, 8287, 124, 43171, 124, 124, 531, 124, 531, 124, 124, 124, 124, 124, 124, 410, 124, 124, 410, 124, 124, 124, 4056, 124, 124, 531, 7163, 124, 124, 124, 531, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 410, 43171, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 45486, 33873, 3768, 18, 18, 18, 18, 18, 33873, 18, 18, 18, 18, 192, 18, 20037, 33873, 1906, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 1906, 1906, 1906, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 1906, 18, 18, 1906, 18, 1906, 18, 18, 1906, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 1906, 18, 18, 18, 18, 18, 18, 18, 18, 1906, 1906, 1906, 18, 18, 18, 18, 1906, 18, 18, 18, 18, 18, 18, 18, 1906, 1906, 18, 18, 18, 18, 1906, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 1906, 18, 18, 18, 1906, 18, 18, 18, 1906, 1906, 18, 10458, 10249, 17162, 8519, 555, 17162, 76, 76, 76, 76, 76, 73, 10249, 76, 76, 76, 76, 76, 76, 76, 76, 1791, 76, 1791, 76, 76, 1791, 3741, 3741, 3741, 76, 3741, 12714, 3741, 12714, 3741, 3741, 76, 76, 73, 76, 76, 76, 76, 48, 76, 76, 76, 10249, 76, 76, 76, 1791, 1791, 48, 1791, 76, 3741, 76, 3741, 76, 3741, 10249, 76, 76, 76, 76, 1791, 76, 76, 10249, 76, 76, 1791, 10249, 76, 76, 76, 1791, 10249, 1791, 10249, 1791, 10249, 76, 76, 10249, 10249, 76, 76, 76, 10249, 603, 603, 603, 603, 603, 76, 603, 603, 10249, 603, 603, 603, 603, 603, 603, 603, 10249, 76, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 76, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 603, 48, 603, 603, 603, 603, 603, 76, 76, 76, 603, 76, 603, 76, 603, 603, 603, 603, 603, 76, 76, 603, 76, 603, 76, 603, 603, 76, 603, 603, 603, 76, 76, 603, 603, 76, 603, 603, 603, 603, 76, 603, 76, 603, 76, 603, 603, 603, 76, 603, 603, 603, 603, 603, 76]\n",
      "2000\n",
      "\tSTARTING EPOCH 1 - LR=[2]...\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\t\tTrain step - Step 30, Loss 0.036568570882081985\n",
      "\t\tRESULT EPOCH 1:\n",
      "\t\t\tTrain Loss: 0.03958378152714835 - Train Accuracy: 0.3275462962962963\n",
      "\n",
      "\tSTARTING EPOCH 2 - LR=[2]...\n",
      "\t\tTrain step - Step 60, Loss 0.02762957476079464\n",
      "\t\tTrain step - Step 90, Loss 0.024917947128415108\n",
      "\t\tRESULT EPOCH 2:\n",
      "\t\t\tTrain Loss: 0.026434935111966398 - Train Accuracy: 0.4742476851851852\n",
      "\n",
      "\tSTARTING EPOCH 3 - LR=[2]...\n",
      "\t\tTrain step - Step 120, Loss 0.024166764691472054\n",
      "\t\tTrain step - Step 150, Loss 0.021699028089642525\n",
      "\t\tRESULT EPOCH 3:\n",
      "\t\t\tTrain Loss: 0.023277177751340247 - Train Accuracy: 0.5477430555555556\n",
      "\n",
      "\tSTARTING EPOCH 4 - LR=[2]...\n",
      "\t\tTrain step - Step 180, Loss 0.022822046652436256\n",
      "\t\tTrain step - Step 210, Loss 0.019914550706744194\n",
      "\t\tRESULT EPOCH 4:\n",
      "\t\t\tTrain Loss: 0.021827776157469662 - Train Accuracy: 0.6157407407407407\n",
      "\n",
      "\tSTARTING EPOCH 5 - LR=[2]...\n",
      "\t\tTrain step - Step 240, Loss 0.024189982563257217\n",
      "\t\tRESULT EPOCH 5:\n",
      "\t\t\tTrain Loss: 0.02086651749495003 - Train Accuracy: 0.6621817129629629\n",
      "\n",
      "\tSTARTING EPOCH 6 - LR=[2]...\n",
      "\t\tTrain step - Step 270, Loss 0.018497483804821968\n",
      "\t\tTrain step - Step 300, Loss 0.02061626873910427\n",
      "\t\tRESULT EPOCH 6:\n",
      "\t\t\tTrain Loss: 0.019845319773863862 - Train Accuracy: 0.6974826388888888\n",
      "\n",
      "\tSTARTING EPOCH 7 - LR=[2]...\n",
      "\t\tTrain step - Step 330, Loss 0.017463114112615585\n",
      "\t\tTrain step - Step 360, Loss 0.016465522348880768\n",
      "\t\tRESULT EPOCH 7:\n",
      "\t\t\tTrain Loss: 0.019022522632170608 - Train Accuracy: 0.7271412037037037\n",
      "\n",
      "\tSTARTING EPOCH 8 - LR=[2]...\n",
      "\t\tTrain step - Step 390, Loss 0.017488008365035057\n",
      "\t\tTrain step - Step 420, Loss 0.019153747707605362\n",
      "\t\tRESULT EPOCH 8:\n",
      "\t\t\tTrain Loss: 0.018500112773229677 - Train Accuracy: 0.7359664351851852\n",
      "\n",
      "\tSTARTING EPOCH 9 - LR=[2]...\n",
      "\t\tTrain step - Step 450, Loss 0.014965708367526531\n",
      "\t\tTrain step - Step 480, Loss 0.01770773157477379\n",
      "\t\tRESULT EPOCH 9:\n",
      "\t\t\tTrain Loss: 0.017725171935227182 - Train Accuracy: 0.7608506944444444\n",
      "\n",
      "\tSTARTING EPOCH 10 - LR=[2]...\n",
      "\t\tTrain step - Step 510, Loss 0.016591094434261322\n",
      "\t\tRESULT EPOCH 10:\n",
      "\t\t\tTrain Loss: 0.017620312929567363 - Train Accuracy: 0.7732928240740741\n",
      "\n",
      "\tSTARTING EPOCH 11 - LR=[2]...\n",
      "\t\tTrain step - Step 540, Loss 0.01652504876255989\n",
      "\t\tTrain step - Step 570, Loss 0.018445458263158798\n",
      "\t\tRESULT EPOCH 11:\n",
      "\t\t\tTrain Loss: 0.017122672870755196 - Train Accuracy: 0.7743055555555556\n",
      "\n",
      "\tSTARTING EPOCH 12 - LR=[2]...\n",
      "\t\tTrain step - Step 600, Loss 0.01606188528239727\n",
      "\t\tTrain step - Step 630, Loss 0.0172007754445076\n",
      "\t\tRESULT EPOCH 12:\n",
      "\t\t\tTrain Loss: 0.016679012389094743 - Train Accuracy: 0.7877604166666666\n",
      "\n",
      "\tSTARTING EPOCH 13 - LR=[2]...\n",
      "\t\tTrain step - Step 660, Loss 0.01522656437009573\n",
      "\t\tTrain step - Step 690, Loss 0.020586984232068062\n",
      "\t\tRESULT EPOCH 13:\n",
      "\t\t\tTrain Loss: 0.01655279303452483 - Train Accuracy: 0.7965856481481481\n",
      "\n",
      "\tSTARTING EPOCH 14 - LR=[2]...\n",
      "\t\tTrain step - Step 720, Loss 0.014289946295320988\n",
      "\t\tTrain step - Step 750, Loss 0.01596979983150959\n",
      "\t\tRESULT EPOCH 14:\n",
      "\t\t\tTrain Loss: 0.01630230185886224 - Train Accuracy: 0.8127893518518519\n",
      "\n",
      "\tSTARTING EPOCH 15 - LR=[2]...\n",
      "\t\tTrain step - Step 780, Loss 0.01396198384463787\n",
      "\t\tRESULT EPOCH 15:\n",
      "\t\t\tTrain Loss: 0.016343156151749468 - Train Accuracy: 0.8120659722222222\n",
      "\n",
      "\tSTARTING EPOCH 16 - LR=[2]...\n",
      "\t\tTrain step - Step 810, Loss 0.01657520793378353\n",
      "\t\tTrain step - Step 840, Loss 0.016315430402755737\n",
      "\t\tRESULT EPOCH 16:\n",
      "\t\t\tTrain Loss: 0.015699358863963023 - Train Accuracy: 0.8148148148148148\n",
      "\n",
      "\tSTARTING EPOCH 17 - LR=[2]...\n",
      "\t\tTrain step - Step 870, Loss 0.018388934433460236\n",
      "\t\tTrain step - Step 900, Loss 0.014546009711921215\n",
      "\t\tRESULT EPOCH 17:\n",
      "\t\t\tTrain Loss: 0.015547080741574367 - Train Accuracy: 0.8219039351851852\n",
      "\n",
      "\tSTARTING EPOCH 18 - LR=[2]...\n",
      "\t\tTrain step - Step 930, Loss 0.014351329766213894\n",
      "\t\tTrain step - Step 960, Loss 0.01609925366938114\n",
      "\t\tRESULT EPOCH 18:\n",
      "\t\t\tTrain Loss: 0.015149242365387854 - Train Accuracy: 0.8339120370370371\n",
      "\n",
      "\tSTARTING EPOCH 19 - LR=[2]...\n",
      "\t\tTrain step - Step 990, Loss 0.012833473272621632\n",
      "\t\tTrain step - Step 1020, Loss 0.015699677169322968\n",
      "\t\tRESULT EPOCH 19:\n",
      "\t\t\tTrain Loss: 0.01498790802779021 - Train Accuracy: 0.8388310185185185\n",
      "\n",
      "\tSTARTING EPOCH 20 - LR=[2]...\n",
      "\t\tTrain step - Step 1050, Loss 0.014246600680053234\n",
      "\t\tRESULT EPOCH 20:\n",
      "\t\t\tTrain Loss: 0.01467334063447736 - Train Accuracy: 0.8391203703703703\n",
      "\n",
      "\tSTARTING EPOCH 21 - LR=[2]...\n",
      "\t\tTrain step - Step 1080, Loss 0.015245242044329643\n",
      "\t\tTrain step - Step 1110, Loss 0.0157717764377594\n",
      "\t\tRESULT EPOCH 21:\n",
      "\t\t\tTrain Loss: 0.015028991357043938 - Train Accuracy: 0.8355034722222222\n",
      "\n",
      "\tSTARTING EPOCH 22 - LR=[2]...\n",
      "\t\tTrain step - Step 1140, Loss 0.01403026282787323\n",
      "\t\tTrain step - Step 1170, Loss 0.0161689855158329\n",
      "\t\tRESULT EPOCH 22:\n",
      "\t\t\tTrain Loss: 0.014513272861087764 - Train Accuracy: 0.8383969907407407\n",
      "\n",
      "\tSTARTING EPOCH 23 - LR=[2]...\n",
      "\t\tTrain step - Step 1200, Loss 0.013972793705761433\n",
      "\t\tTrain step - Step 1230, Loss 0.014473946765065193\n",
      "\t\tRESULT EPOCH 23:\n",
      "\t\t\tTrain Loss: 0.014481603098964249 - Train Accuracy: 0.8460648148148148\n",
      "\n",
      "\tSTARTING EPOCH 24 - LR=[2]...\n",
      "\t\tTrain step - Step 1260, Loss 0.013750985264778137\n",
      "\t\tTrain step - Step 1290, Loss 0.01582019031047821\n",
      "\t\tRESULT EPOCH 24:\n",
      "\t\t\tTrain Loss: 0.014465709441100006 - Train Accuracy: 0.8573495370370371\n",
      "\n",
      "\tSTARTING EPOCH 25 - LR=[2]...\n",
      "\t\tTrain step - Step 1320, Loss 0.012903695926070213\n",
      "\t\tRESULT EPOCH 25:\n",
      "\t\t\tTrain Loss: 0.014207894603411356 - Train Accuracy: 0.8621238425925926\n",
      "\n",
      "\tSTARTING EPOCH 26 - LR=[2]...\n",
      "\t\tTrain step - Step 1350, Loss 0.011553101241588593\n",
      "\t\tTrain step - Step 1380, Loss 0.013711931183934212\n",
      "\t\tRESULT EPOCH 26:\n",
      "\t\t\tTrain Loss: 0.013995140721952473 - Train Accuracy: 0.8577835648148148\n",
      "\n",
      "\tSTARTING EPOCH 27 - LR=[2]...\n",
      "\t\tTrain step - Step 1410, Loss 0.013768274337053299\n",
      "\t\tTrain step - Step 1440, Loss 0.013487438671290874\n",
      "\t\tRESULT EPOCH 27:\n",
      "\t\t\tTrain Loss: 0.013997615832421515 - Train Accuracy: 0.8641493055555556\n",
      "\n",
      "\tSTARTING EPOCH 28 - LR=[2]...\n",
      "\t\tTrain step - Step 1470, Loss 0.01268843188881874\n",
      "\t\tTrain step - Step 1500, Loss 0.014125900343060493\n",
      "\t\tRESULT EPOCH 28:\n",
      "\t\t\tTrain Loss: 0.013210273365041724 - Train Accuracy: 0.8736979166666666\n",
      "\n",
      "\tSTARTING EPOCH 29 - LR=[2]...\n",
      "\t\tTrain step - Step 1530, Loss 0.013998427428305149\n",
      "\t\tTrain step - Step 1560, Loss 0.014622300863265991\n",
      "\t\tRESULT EPOCH 29:\n",
      "\t\t\tTrain Loss: 0.013891415222099534 - Train Accuracy: 0.8667534722222222\n",
      "\n",
      "\tSTARTING EPOCH 30 - LR=[2]...\n",
      "\t\tTrain step - Step 1590, Loss 0.014626474119722843\n",
      "\t\tRESULT EPOCH 30:\n",
      "\t\t\tTrain Loss: 0.013960564371060443 - Train Accuracy: 0.8723958333333334\n",
      "\n",
      "\tSTARTING EPOCH 31 - LR=[2]...\n",
      "\t\tTrain step - Step 1620, Loss 0.014346806332468987\n",
      "\t\tTrain step - Step 1650, Loss 0.011682434938848019\n",
      "\t\tRESULT EPOCH 31:\n",
      "\t\t\tTrain Loss: 0.013348477271695932 - Train Accuracy: 0.8732638888888888\n",
      "\n",
      "\tSTARTING EPOCH 32 - LR=[2]...\n",
      "\t\tTrain step - Step 1680, Loss 0.011501507833600044\n",
      "\t\tTrain step - Step 1710, Loss 0.012422255240380764\n",
      "\t\tRESULT EPOCH 32:\n",
      "\t\t\tTrain Loss: 0.013179262530886464 - Train Accuracy: 0.8804976851851852\n",
      "\n",
      "\tSTARTING EPOCH 33 - LR=[2]...\n",
      "\t\tTrain step - Step 1740, Loss 0.013294405303895473\n",
      "\t\tTrain step - Step 1770, Loss 0.014354872517287731\n",
      "\t\tRESULT EPOCH 33:\n",
      "\t\t\tTrain Loss: 0.013239697277269981 - Train Accuracy: 0.8783275462962963\n",
      "\n",
      "\tSTARTING EPOCH 34 - LR=[2]...\n",
      "\t\tTrain step - Step 1800, Loss 0.014191687107086182\n",
      "\t\tTrain step - Step 1830, Loss 0.01415176596492529\n",
      "\t\tRESULT EPOCH 34:\n",
      "\t\t\tTrain Loss: 0.013339288084319344 - Train Accuracy: 0.8833912037037037\n",
      "\n",
      "\tSTARTING EPOCH 35 - LR=[2]...\n",
      "\t\tTrain step - Step 1860, Loss 0.012211725115776062\n",
      "\t\tRESULT EPOCH 35:\n",
      "\t\t\tTrain Loss: 0.01298557511634297 - Train Accuracy: 0.8927951388888888\n",
      "\n",
      "\tSTARTING EPOCH 36 - LR=[2]...\n",
      "\t\tTrain step - Step 1890, Loss 0.014100385829806328\n",
      "\t\tTrain step - Step 1920, Loss 0.01259373314678669\n",
      "\t\tRESULT EPOCH 36:\n",
      "\t\t\tTrain Loss: 0.012956532097801014 - Train Accuracy: 0.8862847222222222\n",
      "\n",
      "\tSTARTING EPOCH 37 - LR=[2]...\n",
      "\t\tTrain step - Step 1950, Loss 0.012576507404446602\n",
      "\t\tTrain step - Step 1980, Loss 0.01207686122506857\n",
      "\t\tRESULT EPOCH 37:\n",
      "\t\t\tTrain Loss: 0.012877588401790018 - Train Accuracy: 0.8870081018518519\n",
      "\n",
      "\tSTARTING EPOCH 38 - LR=[2]...\n",
      "\t\tTrain step - Step 2010, Loss 0.011776277795433998\n",
      "\t\tTrain step - Step 2040, Loss 0.013787650503218174\n",
      "\t\tRESULT EPOCH 38:\n",
      "\t\t\tTrain Loss: 0.012745736580755975 - Train Accuracy: 0.8956886574074074\n",
      "\n",
      "\tSTARTING EPOCH 39 - LR=[2]...\n",
      "\t\tTrain step - Step 2070, Loss 0.012698225677013397\n",
      "\t\tTrain step - Step 2100, Loss 0.012447685934603214\n",
      "\t\tRESULT EPOCH 39:\n",
      "\t\t\tTrain Loss: 0.012794847927849603 - Train Accuracy: 0.8833912037037037\n",
      "\n",
      "\tSTARTING EPOCH 40 - LR=[2]...\n",
      "\t\tTrain step - Step 2130, Loss 0.014182763174176216\n",
      "\t\tRESULT EPOCH 40:\n",
      "\t\t\tTrain Loss: 0.013096756284573564 - Train Accuracy: 0.8878761574074074\n",
      "\n",
      "\tSTARTING EPOCH 41 - LR=[2]...\n",
      "\t\tTrain step - Step 2160, Loss 0.01228189468383789\n",
      "\t\tTrain step - Step 2190, Loss 0.01239231787621975\n",
      "\t\tRESULT EPOCH 41:\n",
      "\t\t\tTrain Loss: 0.01294784563490086 - Train Accuracy: 0.8961226851851852\n",
      "\n",
      "\tSTARTING EPOCH 42 - LR=[2]...\n",
      "\t\tTrain step - Step 2220, Loss 0.011765990406274796\n",
      "\t\tTrain step - Step 2250, Loss 0.010659183375537395\n",
      "\t\tRESULT EPOCH 42:\n",
      "\t\t\tTrain Loss: 0.012631143943441135 - Train Accuracy: 0.9010416666666666\n",
      "\n",
      "\tSTARTING EPOCH 43 - LR=[2]...\n",
      "\t\tTrain step - Step 2280, Loss 0.012234223075211048\n",
      "\t\tTrain step - Step 2310, Loss 0.01155597623437643\n",
      "\t\tRESULT EPOCH 43:\n",
      "\t\t\tTrain Loss: 0.012377995359538882 - Train Accuracy: 0.9001736111111112\n",
      "\n",
      "\tSTARTING EPOCH 44 - LR=[2]...\n",
      "\t\tTrain step - Step 2340, Loss 0.011957001872360706\n",
      "\t\tTrain step - Step 2370, Loss 0.012834800407290459\n",
      "\t\tRESULT EPOCH 44:\n",
      "\t\t\tTrain Loss: 0.012348228306682021 - Train Accuracy: 0.9042245370370371\n",
      "\n",
      "\tSTARTING EPOCH 45 - LR=[2]...\n",
      "\t\tTrain step - Step 2400, Loss 0.01177267637103796\n",
      "\t\tRESULT EPOCH 45:\n",
      "\t\t\tTrain Loss: 0.012502061703276855 - Train Accuracy: 0.9036458333333334\n",
      "\n",
      "\tSTARTING EPOCH 46 - LR=[2]...\n",
      "\t\tTrain step - Step 2430, Loss 0.011479558423161507\n",
      "\t\tTrain step - Step 2460, Loss 0.011397366411983967\n",
      "\t\tRESULT EPOCH 46:\n",
      "\t\t\tTrain Loss: 0.012307611069883461 - Train Accuracy: 0.8995949074074074\n",
      "\n",
      "\tSTARTING EPOCH 47 - LR=[2]...\n",
      "\t\tTrain step - Step 2490, Loss 0.014029715210199356\n",
      "\t\tTrain step - Step 2520, Loss 0.012976846657693386\n",
      "\t\tRESULT EPOCH 47:\n",
      "\t\t\tTrain Loss: 0.012525648534021995 - Train Accuracy: 0.9011863425925926\n",
      "\n",
      "\tSTARTING EPOCH 48 - LR=[2]...\n",
      "\t\tTrain step - Step 2550, Loss 0.014020811766386032\n",
      "\t\tTrain step - Step 2580, Loss 0.01164679229259491\n",
      "\t\tRESULT EPOCH 48:\n",
      "\t\t\tTrain Loss: 0.012340496207966848 - Train Accuracy: 0.9052372685185185\n",
      "\n",
      "\tSTARTING EPOCH 49 - LR=[2]...\n",
      "\t\tTrain step - Step 2610, Loss 0.011980312876403332\n",
      "\t\tTrain step - Step 2640, Loss 0.012113832868635654\n",
      "\t\tRESULT EPOCH 49:\n",
      "\t\t\tTrain Loss: 0.011955398928236079 - Train Accuracy: 0.9113136574074074\n",
      "\n",
      "\tSTARTING EPOCH 50 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2670, Loss 0.010920407250523567\n",
      "\t\tRESULT EPOCH 50:\n",
      "\t\t\tTrain Loss: 0.010596624299607895 - Train Accuracy: 0.9289641203703703\n",
      "\n",
      "\tSTARTING EPOCH 51 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2700, Loss 0.010156714357435703\n",
      "\t\tTrain step - Step 2730, Loss 0.008700111880898476\n",
      "\t\tRESULT EPOCH 51:\n",
      "\t\t\tTrain Loss: 0.009635047797389605 - Train Accuracy: 0.9409722222222222\n",
      "\n",
      "\tSTARTING EPOCH 52 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2760, Loss 0.008229533210396767\n",
      "\t\tTrain step - Step 2790, Loss 0.008546235039830208\n",
      "\t\tRESULT EPOCH 52:\n",
      "\t\t\tTrain Loss: 0.009349919355439919 - Train Accuracy: 0.9438657407407407\n",
      "\n",
      "\tSTARTING EPOCH 53 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2820, Loss 0.008663936518132687\n",
      "\t\tTrain step - Step 2850, Loss 0.009495577774941921\n",
      "\t\tRESULT EPOCH 53:\n",
      "\t\t\tTrain Loss: 0.009343690601074033 - Train Accuracy: 0.9425636574074074\n",
      "\n",
      "\tSTARTING EPOCH 54 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2880, Loss 0.009835273958742619\n",
      "\t\tTrain step - Step 2910, Loss 0.008843889459967613\n",
      "\t\tRESULT EPOCH 54:\n",
      "\t\t\tTrain Loss: 0.009247544902825245 - Train Accuracy: 0.9427083333333334\n",
      "\n",
      "\tSTARTING EPOCH 55 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2940, Loss 0.00975545309484005\n",
      "\t\tRESULT EPOCH 55:\n",
      "\t\t\tTrain Loss: 0.009222983882796985 - Train Accuracy: 0.9456018518518519\n",
      "\n",
      "\tSTARTING EPOCH 56 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2970, Loss 0.008757675997912884\n",
      "\t\tTrain step - Step 3000, Loss 0.010005466639995575\n",
      "\t\tRESULT EPOCH 56:\n",
      "\t\t\tTrain Loss: 0.009140247815392084 - Train Accuracy: 0.9444444444444444\n",
      "\n",
      "\tSTARTING EPOCH 57 - LR=[0.4]...\n",
      "\t\tTrain step - Step 3030, Loss 0.008528481237590313\n",
      "\t\tTrain step - Step 3060, Loss 0.009737336076796055\n",
      "\t\tRESULT EPOCH 57:\n",
      "\t\t\tTrain Loss: 0.008951247988820629 - Train Accuracy: 0.9476273148148148\n",
      "\n",
      "\tSTARTING EPOCH 58 - LR=[0.4]...\n",
      "\t\tTrain step - Step 3090, Loss 0.00994714442640543\n",
      "\t\tTrain step - Step 3120, Loss 0.009010481648147106\n",
      "\t\tRESULT EPOCH 58:\n",
      "\t\t\tTrain Loss: 0.008895093609613401 - Train Accuracy: 0.9477719907407407\n",
      "\n",
      "\tSTARTING EPOCH 59 - LR=[0.4]...\n",
      "\t\tTrain step - Step 3150, Loss 0.008253881707787514\n",
      "\t\tTrain step - Step 3180, Loss 0.008247350342571735\n",
      "\t\tRESULT EPOCH 59:\n",
      "\t\t\tTrain Loss: 0.008938585468395441 - Train Accuracy: 0.9508101851851852\n",
      "\n",
      "\tSTARTING EPOCH 60 - LR=[0.4]...\n",
      "\t\tTrain step - Step 3210, Loss 0.008241563104093075\n",
      "\t\tRESULT EPOCH 60:\n",
      "\t\t\tTrain Loss: 0.008828045377783754 - Train Accuracy: 0.9476273148148148\n",
      "\n",
      "\tSTARTING EPOCH 61 - LR=[0.4]...\n",
      "\t\tTrain step - Step 3240, Loss 0.008309360593557358\n",
      "\t\tTrain step - Step 3270, Loss 0.009187420830130577\n",
      "\t\tRESULT EPOCH 61:\n",
      "\t\t\tTrain Loss: 0.00865477431324069 - Train Accuracy: 0.9496527777777778\n",
      "\n",
      "\tSTARTING EPOCH 62 - LR=[0.4]...\n",
      "\t\tTrain step - Step 3300, Loss 0.009916739538311958\n",
      "\t\tTrain step - Step 3330, Loss 0.008902427740395069\n",
      "\t\tRESULT EPOCH 62:\n",
      "\t\t\tTrain Loss: 0.008747944004695725 - Train Accuracy: 0.9480613425925926\n",
      "\n",
      "\tSTARTING EPOCH 63 - LR=[0.4]...\n",
      "\t\tTrain step - Step 3360, Loss 0.008969580754637718\n",
      "\t\tTrain step - Step 3390, Loss 0.00888406578451395\n",
      "\t\tRESULT EPOCH 63:\n",
      "\t\t\tTrain Loss: 0.008773633097815845 - Train Accuracy: 0.9515335648148148\n",
      "\n",
      "\tSTARTING EPOCH 64 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 3420, Loss 0.008406917564570904\n",
      "\t\tTrain step - Step 3450, Loss 0.008149875327944756\n",
      "\t\tRESULT EPOCH 64:\n",
      "\t\t\tTrain Loss: 0.008567478456017043 - Train Accuracy: 0.9542824074074074\n",
      "\n",
      "\tSTARTING EPOCH 65 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 3480, Loss 0.009015210904181004\n",
      "\t\tRESULT EPOCH 65:\n",
      "\t\t\tTrain Loss: 0.00855908982662691 - Train Accuracy: 0.94921875\n",
      "\n",
      "\tSTARTING EPOCH 66 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 3510, Loss 0.00722850114107132\n",
      "\t\tTrain step - Step 3540, Loss 0.00796679500490427\n",
      "\t\tRESULT EPOCH 66:\n",
      "\t\t\tTrain Loss: 0.008524685537580538 - Train Accuracy: 0.9519675925925926\n",
      "\n",
      "\tSTARTING EPOCH 67 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 3570, Loss 0.0079717468470335\n",
      "\t\tTrain step - Step 3600, Loss 0.007885622791945934\n",
      "\t\tRESULT EPOCH 67:\n",
      "\t\t\tTrain Loss: 0.008442014809352931 - Train Accuracy: 0.9513888888888888\n",
      "\n",
      "\tSTARTING EPOCH 68 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 3630, Loss 0.00673852302134037\n",
      "\t\tTrain step - Step 3660, Loss 0.008565131574869156\n",
      "\t\tRESULT EPOCH 68:\n",
      "\t\t\tTrain Loss: 0.008538246585953015 - Train Accuracy: 0.9560185185185185\n",
      "\n",
      "\tSTARTING EPOCH 69 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 3690, Loss 0.00784995499998331\n",
      "\t\tTrain step - Step 3720, Loss 0.008870701305568218\n",
      "\t\tRESULT EPOCH 69:\n",
      "\t\t\tTrain Loss: 0.008418102912535821 - Train Accuracy: 0.9506655092592593\n",
      "\n",
      "\tSTARTING EPOCH 70 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 3750, Loss 0.00829818844795227\n",
      "\t\tRESULT EPOCH 70:\n",
      "\t\t\tTrain Loss: 0.00833645631145272 - Train Accuracy: 0.9537037037037037\n",
      "\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/16 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "TEEEEEEEEEEEEST\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:00<00:00, 16.77it/s]\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\tResults STAGE 2:\n",
      "\t\tTrain Mean Accuracy: 0.8557002314814814\n",
      "\t\tTest Accuracy: 0.7375\n",
      "\n",
      "STARTING FINE TUNING STAGE 3...\n",
      "[37732, 509, 509, 2020, 57, 9587, 118, 14710, 8023, 2020, 14710, 14710, 57, 57, 57, 57, 509, 14710, 57, 14710, 57, 9587, 57, 9587, 509, 509, 57, 10234, 57, 9587, 57, 57, 9587, 9587, 57, 9587, 10234, 57, 509, 57, 57, 9587, 9587, 57, 4623, 57, 4623, 9587, 9587, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 33643, 3709, 449, 449, 449, 449, 449, 985, 985, 3636, 48697, 449, 449, 3620, 449, 3620, 3620, 3636, 3620, 3620, 30006, 449, 313, 313, 36355, 372, 313, 313, 449, 36355, 36355, 36355, 449, 313, 313, 313, 1532, 313, 313, 313, 1532, 36355, 313, 1532, 313, 313, 36355, 313, 313, 1532, 36355, 313, 313, 313, 313, 313, 313, 36355, 313, 313, 1532, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 313, 449, 449, 313, 313, 313, 313, 313, 313, 43149, 8949, 48806, 8949, 911, 48806, 120, 120, 13416, 120, 120, 439, 120, 120, 120, 120, 1292, 120, 13416, 1292, 120, 120, 13416, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 1292, 120, 120, 120, 1292, 1292, 120, 1292, 120, 1292, 1292, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 1292, 120, 1292, 1292, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 27295, 2057, 6943, 1726, 27, 27, 27, 27, 27, 27, 7172, 1726, 1726, 1726, 14145, 27, 27, 27, 27, 27, 27, 27, 27, 27, 8302, 27, 27, 27, 27, 4661, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 24723, 2852, 5375, 219, 219, 31381, 219, 9101, 31381, 16, 575, 575, 16, 16, 16, 16, 16, 4221, 5375, 16, 7101, 16, 16, 16, 219, 16, 16, 219, 5375, 1255, 16, 16, 16, 16, 7101, 16, 16, 16, 16, 219, 219, 219, 219, 16, 219, 219, 16, 16, 219, 16, 16, 219, 16, 219, 16, 16, 219, 31381, 16, 16, 16, 16, 219, 219, 219, 16, 219, 219, 219, 4448, 219, 4448, 16, 16, 16, 4448, 219, 4448, 16, 30, 16, 16, 4448, 219, 4448, 4448, 4448, 219, 4448, 4448, 30, 219, 30, 16, 4448, 4448, 16, 219, 16, 16, 29770, 25520, 148, 12106, 23617, 148, 148, 148, 148, 36079, 148, 20, 536, 148, 536, 148, 148, 148, 148, 148, 148, 148, 665, 25520, 148, 665, 665, 148, 148, 25520, 665, 20, 25520, 665, 20, 20, 20, 20, 20, 20, 25520, 665, 665, 20, 20, 25520, 148, 20, 148, 20, 20, 20, 20, 20, 20, 148, 148, 20, 148, 20, 148, 20, 20, 20, 20, 20, 20, 25520, 20, 20, 148, 25520, 20, 20, 148, 148, 148, 25520, 20, 20, 20, 20, 20, 20, 20, 20, 20, 148, 20, 20, 20, 25520, 20, 20, 20, 20, 20, 20, 25520, 20, 13806, 1558, 6938, 1558, 6938, 1558, 6938, 172, 1558, 24295, 43986, 34, 172, 172, 34, 172, 34, 172, 34, 24295, 1558, 172, 1558, 34, 172, 25397, 25397, 25397, 34, 25397, 25397, 34, 34, 34, 34, 172, 172, 172, 172, 172, 34, 172, 172, 172, 172, 34, 34, 34, 34, 34, 172, 34, 34, 172, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 172, 34, 172, 172, 34, 34, 34, 34, 172, 172, 34, 34, 34, 34, 172, 34, 172, 172, 34, 34, 34, 172, 34, 172, 172, 34, 34, 172, 34, 34, 34, 34, 172, 34, 172, 44516, 7163, 4056, 124, 4614, 1789, 4056, 2774, 4056, 4056, 14205, 14205, 26774, 8287, 43171, 4056, 124, 124, 124, 124, 124, 124, 8287, 124, 43171, 124, 124, 531, 124, 531, 124, 124, 124, 124, 124, 124, 410, 124, 124, 410, 124, 124, 124, 4056, 124, 124, 531, 7163, 124, 124, 124, 531, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 410, 43171, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 45486, 33873, 3768, 18, 18, 18, 18, 18, 33873, 18, 18, 18, 18, 192, 18, 20037, 33873, 1906, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 1906, 1906, 1906, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 1906, 18, 18, 1906, 18, 1906, 18, 18, 1906, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 10458, 10249, 17162, 8519, 555, 17162, 76, 76, 76, 76, 76, 73, 10249, 76, 76, 76, 76, 76, 76, 76, 76, 1791, 76, 1791, 76, 76, 1791, 3741, 3741, 3741, 76, 3741, 12714, 3741, 12714, 3741, 3741, 76, 76, 73, 76, 76, 76, 76, 48, 76, 76, 76, 10249, 76, 76, 76, 1791, 1791, 48, 1791, 76, 3741, 76, 3741, 76, 3741, 10249, 76, 76, 76, 76, 1791, 76, 76, 10249, 76, 76, 1791, 10249, 76, 76, 76, 1791, 10249, 1791, 10249, 1791, 10249, 76, 76, 10249, 10249, 76, 76, 76, 10249, 603, 603, 603, 603, 603, 76, 603, 603, 11449, 25771, 25771, 12, 12, 274, 274, 12, 12, 25771, 12, 466, 7442, 9746, 9746, 9746, 7442, 16784, 43293, 164, 12, 12, 12, 43293, 12, 8573, 16784, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 274, 12, 12, 12, 274, 12, 274, 12, 12, 12, 12, 9746, 12, 12, 12, 12, 12, 12, 12, 164, 164, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 274, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 274, 12, 12, 12, 12, 12, 12, 274, 12, 12, 274, 12, 12, 12, 12, 12, 12, 9746, 37752, 14789, 243, 476, 14789, 1193, 6184, 1113, 1113, 14789, 243, 243, 243, 956, 956, 956, 956, 13949, 14094, 83, 1438, 83, 1438, 243, 243, 83, 83, 83, 83, 243, 243, 956, 14789, 83, 83, 83, 476, 476, 476, 476, 83, 83, 83, 476, 476, 83, 476, 83, 83, 83, 83, 83, 83, 83, 83, 83, 243, 117, 243, 83, 83, 83, 117, 117, 83, 83, 117, 14094, 14094, 83, 117, 117, 14094, 243, 83, 243, 83, 83, 83, 83, 14094, 83, 83, 243, 243, 83, 243, 83, 83, 83, 83, 14094, 83, 117, 14094, 14094, 243, 83, 956, 83, 46840, 6389, 658, 304, 789, 789, 12738, 176, 176, 3458, 176, 176, 176, 176, 176, 176, 42834, 9470, 2, 176, 176, 2, 176, 17036, 2, 2, 32159, 2, 2, 2, 2, 2, 2, 3007, 2, 2, 2, 2, 2, 2, 2, 176, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3007, 3007, 32159, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 176, 2, 3007, 2, 3007, 3007, 2, 2, 176, 2, 2, 2, 2, 176, 176, 2, 176, 2, 2, 3007, 24028, 8342, 17033, 29624, 634, 634, 634, 29624, 6676, 87, 87, 8342, 87, 634, 87, 634, 8342, 87, 634, 87, 87, 87, 1692, 87, 87, 8342, 28085, 87, 87, 87, 87, 87, 87, 991, 87, 87, 87, 634, 87, 28085, 991, 87, 87, 634, 87, 655, 87, 87, 87, 634, 87, 87, 4230, 87, 1692, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 991, 991, 991, 634, 87, 87, 634, 87, 991, 991, 87, 87, 87, 991, 87, 655, 87, 28085, 87, 87, 991, 42309, 18459, 10580, 5000, 79, 79, 4030, 949, 79, 79, 29861, 79, 79, 79, 79, 28, 79, 28, 79, 79, 949, 79, 79, 79, 17331, 17331, 28, 28, 28, 28, 28, 28, 28, 79, 28, 28, 79, 28, 28, 28, 28, 28, 28, 28, 28, 79, 79, 79, 28, 28, 28, 28, 28, 28, 18792, 79, 79, 28, 28, 28, 28, 18792, 79, 79, 28, 28, 28, 79, 18792, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 28, 18792, 28, 18792, 28, 28, 28, 28, 28, 18792, 79, 79, 79, 79, 79, 79, 79, 79, 79, 28, 35415, 33228, 426, 394, 191, 426, 426, 6455, 2742, 191, 191, 426, 1392, 426, 191, 394, 6455, 125, 34332, 125, 13963, 125, 125, 125, 125, 2974, 125, 125, 2974, 125, 4863, 2974, 125, 125, 125, 125, 125, 125, 125, 125, 125, 8323, 426, 715, 8323, 715, 426, 125, 125, 125, 125, 125, 125, 125, 125, 715, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 4863, 125, 2974, 125, 125, 394, 125, 2974, 125, 2974, 125, 125, 4863, 125, 125, 125, 2974, 125, 4863, 4863, 394, 125, 125, 125, 125, 394, 125, 394, 394, 125, 125, 125, 125, 6449, 37234, 717, 2116, 37234, 717, 717, 116, 1673, 116, 415, 415, 717, 415, 717, 1855, 279, 279, 1673, 932, 116, 209, 116, 415, 717, 717, 717, 415, 717, 1185, 717, 717, 8173, 116, 1185, 1185, 116, 2450, 2450, 116, 717, 717, 116, 116, 116, 116, 37234, 116, 320, 116, 320, 116, 116, 116, 116, 116, 116, 415, 415, 116, 116, 116, 116, 415, 415, 116, 42651, 116, 116, 116, 116, 116, 116, 116, 116, 116, 415, 42651, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 116, 415, 415, 116, 116, 116, 415, 415, 415, 116, 116, 12204, 7021, 7021, 7021, 7021, 751, 1547, 1547, 1547, 1547, 6672, 7021, 7021, 6044, 7021, 419, 419, 650, 419, 419, 4227, 650, 13629, 3279, 419, 1547, 70, 70, 13629, 70, 70, 70, 3279, 650, 70, 1547, 70, 1547, 70, 70, 1547, 70, 650, 6044, 751, 70, 751, 1547, 70, 70, 70, 1547, 1547, 1547, 70, 7021, 7021, 1056, 1112, 1547, 1112, 1112, 7021, 1112, 1112, 1547, 1547, 70, 70, 7021, 70, 7021, 1547, 70, 7021, 7021, 1112, 1112, 1112, 70, 70, 1547, 1547, 70, 1112, 1112, 7021, 1112, 70, 1112, 1112, 70, 70, 70, 70, 1112, 1112, 7021, 1112, 70, 36150, 252, 7976, 252, 252, 252, 252, 252, 576, 252, 7976, 576, 576, 1841, 2499, 11975, 152, 576, 576, 195, 195, 195, 576, 152, 46724, 152, 251, 152, 152, 152, 152, 152, 152, 408, 1965, 152, 252, 152, 152, 251, 252, 152, 3817, 152, 152, 251, 408, 251, 251, 251, 251, 251, 152, 251, 251, 152, 152, 251, 152, 251, 251, 251, 251, 251, 152, 152, 251, 251, 251, 152, 251, 152, 576, 152, 152, 251, 152, 576, 251, 251, 251, 251, 152, 152, 251, 251, 152, 251, 576, 152, 152, 576, 576, 576, 251, 251, 251, 152, 576, 576, 385, 37570, 4908, 21167, 889, 307, 30225, 244, 244, 4908, 244, 244, 244, 7443, 244, 244, 244, 244, 244, 244, 4908, 244, 9386, 12266, 244, 244, 244, 7443, 7443, 7443, 7443, 7443, 7443, 244, 7443, 244, 244, 244, 7443, 7443, 307, 244, 244, 7443, 244, 244, 7443, 244, 7443, 244, 244, 244, 7443, 244, 7443, 244, 7443, 244, 244, 7443, 307, 244, 7443, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244, 244]\n",
      "2000\n",
      "\tSTARTING EPOCH 1 - LR=[2]...\n",
      "\t\tTrain step - Step 30, Loss 0.038144562393426895\n",
      "\t\tRESULT EPOCH 1:\n",
      "\t\t\tTrain Loss: 0.0436896883227207 - Train Accuracy: 0.30150462962962965\n",
      "\n",
      "\tSTARTING EPOCH 2 - LR=[2]...\n",
      "\t\tTrain step - Step 60, Loss 0.033533837646245956\n",
      "\t\tTrain step - Step 90, Loss 0.03090791217982769\n",
      "\t\tRESULT EPOCH 2:\n",
      "\t\t\tTrain Loss: 0.034804852920825836 - Train Accuracy: 0.39076967592592593\n",
      "\n",
      "\tSTARTING EPOCH 3 - LR=[2]...\n",
      "\t\tTrain step - Step 120, Loss 0.029844855889678\n",
      "\t\tTrain step - Step 150, Loss 0.033562954515218735\n",
      "\t\tRESULT EPOCH 3:\n",
      "\t\t\tTrain Loss: 0.032267285890325355 - Train Accuracy: 0.44458912037037035\n",
      "\n",
      "\tSTARTING EPOCH 4 - LR=[2]...\n",
      "\t\tTrain step - Step 180, Loss 0.03118770383298397\n",
      "\t\tTrain step - Step 210, Loss 0.0314946323633194\n",
      "\t\tRESULT EPOCH 4:\n",
      "\t\t\tTrain Loss: 0.03143410826170886 - Train Accuracy: 0.48119212962962965\n",
      "\n",
      "\tSTARTING EPOCH 5 - LR=[2]...\n",
      "\t\tTrain step - Step 240, Loss 0.02939286082983017\n",
      "\t\tRESULT EPOCH 5:\n",
      "\t\t\tTrain Loss: 0.030860199489527278 - Train Accuracy: 0.5188078703703703\n",
      "\n",
      "\tSTARTING EPOCH 6 - LR=[2]...\n",
      "\t\tTrain step - Step 270, Loss 0.030774587765336037\n",
      "\t\tTrain step - Step 300, Loss 0.03071202151477337\n",
      "\t\tRESULT EPOCH 6:\n",
      "\t\t\tTrain Loss: 0.029899183522771905 - Train Accuracy: 0.5410879629629629\n",
      "\n",
      "\tSTARTING EPOCH 7 - LR=[2]...\n",
      "\t\tTrain step - Step 330, Loss 0.028362572193145752\n",
      "\t\tTrain step - Step 360, Loss 0.030513525009155273\n",
      "\t\tRESULT EPOCH 7:\n",
      "\t\t\tTrain Loss: 0.02955240273365268 - Train Accuracy: 0.5704571759259259\n",
      "\n",
      "\tSTARTING EPOCH 8 - LR=[2]...\n",
      "\t\tTrain step - Step 390, Loss 0.02726384624838829\n",
      "\t\tTrain step - Step 420, Loss 0.027392176911234856\n",
      "\t\tRESULT EPOCH 8:\n",
      "\t\t\tTrain Loss: 0.029235282085008092 - Train Accuracy: 0.5823206018518519\n",
      "\n",
      "\tSTARTING EPOCH 9 - LR=[2]...\n",
      "\t\tTrain step - Step 450, Loss 0.027919121086597443\n",
      "\t\tTrain step - Step 480, Loss 0.028715262189507484\n",
      "\t\tRESULT EPOCH 9:\n",
      "\t\t\tTrain Loss: 0.02890778198424313 - Train Accuracy: 0.6047453703703703\n",
      "\n",
      "\tSTARTING EPOCH 10 - LR=[2]...\n",
      "\t\tTrain step - Step 510, Loss 0.029384830966591835\n",
      "\t\tRESULT EPOCH 10:\n",
      "\t\t\tTrain Loss: 0.028702813476600027 - Train Accuracy: 0.6090856481481481\n",
      "\n",
      "\tSTARTING EPOCH 11 - LR=[2]...\n",
      "\t\tTrain step - Step 540, Loss 0.026099609211087227\n",
      "\t\tTrain step - Step 570, Loss 0.029548754915595055\n",
      "\t\tRESULT EPOCH 11:\n",
      "\t\t\tTrain Loss: 0.028503742137992824 - Train Accuracy: 0.6280381944444444\n",
      "\n",
      "\tSTARTING EPOCH 12 - LR=[2]...\n",
      "\t\tTrain step - Step 600, Loss 0.027779201045632362\n",
      "\t\tTrain step - Step 630, Loss 0.026795290410518646\n",
      "\t\tRESULT EPOCH 12:\n",
      "\t\t\tTrain Loss: 0.02779075568886819 - Train Accuracy: 0.6504629629629629\n",
      "\n",
      "\tSTARTING EPOCH 13 - LR=[2]...\n",
      "\t\tTrain step - Step 660, Loss 0.026777023449540138\n",
      "\t\tTrain step - Step 690, Loss 0.027019312605261803\n",
      "\t\tRESULT EPOCH 13:\n",
      "\t\t\tTrain Loss: 0.02787737306897287 - Train Accuracy: 0.6514756944444444\n",
      "\n",
      "\tSTARTING EPOCH 14 - LR=[2]...\n",
      "\t\tTrain step - Step 720, Loss 0.025790389627218246\n",
      "\t\tTrain step - Step 750, Loss 0.026512250304222107\n",
      "\t\tRESULT EPOCH 14:\n",
      "\t\t\tTrain Loss: 0.027550967168752796 - Train Accuracy: 0.6665219907407407\n",
      "\n",
      "\tSTARTING EPOCH 15 - LR=[2]...\n",
      "\t\tTrain step - Step 780, Loss 0.029601464048027992\n",
      "\t\tRESULT EPOCH 15:\n",
      "\t\t\tTrain Loss: 0.027231022543101398 - Train Accuracy: 0.6765046296296297\n",
      "\n",
      "\tSTARTING EPOCH 16 - LR=[2]...\n",
      "\t\tTrain step - Step 810, Loss 0.02502484805881977\n",
      "\t\tTrain step - Step 840, Loss 0.02742612548172474\n",
      "\t\tRESULT EPOCH 16:\n",
      "\t\t\tTrain Loss: 0.02696708006853307 - Train Accuracy: 0.6850405092592593\n",
      "\n",
      "\tSTARTING EPOCH 17 - LR=[2]...\n",
      "\t\tTrain step - Step 870, Loss 0.025163304060697556\n",
      "\t\tTrain step - Step 900, Loss 0.02939777635037899\n",
      "\t\tRESULT EPOCH 17:\n",
      "\t\t\tTrain Loss: 0.027137924917042255 - Train Accuracy: 0.6885127314814815\n",
      "\n",
      "\tSTARTING EPOCH 18 - LR=[2]...\n",
      "\t\tTrain step - Step 930, Loss 0.027452515438199043\n",
      "\t\tTrain step - Step 960, Loss 0.029133375734090805\n",
      "\t\tRESULT EPOCH 18:\n",
      "\t\t\tTrain Loss: 0.026712318744372437 - Train Accuracy: 0.7025462962962963\n",
      "\n",
      "\tSTARTING EPOCH 19 - LR=[2]...\n",
      "\t\tTrain step - Step 990, Loss 0.025754384696483612\n",
      "\t\tTrain step - Step 1020, Loss 0.027235407382249832\n",
      "\t\tRESULT EPOCH 19:\n",
      "\t\t\tTrain Loss: 0.026646879260186798 - Train Accuracy: 0.7097800925925926\n",
      "\n",
      "\tSTARTING EPOCH 20 - LR=[2]...\n",
      "\t\tTrain step - Step 1050, Loss 0.02507064677774906\n",
      "\t\tRESULT EPOCH 20:\n",
      "\t\t\tTrain Loss: 0.026324144840516425 - Train Accuracy: 0.7188946759259259\n",
      "\n",
      "\tSTARTING EPOCH 21 - LR=[2]...\n",
      "\t\tTrain step - Step 1080, Loss 0.02306465245783329\n",
      "\t\tTrain step - Step 1110, Loss 0.025892462581396103\n",
      "\t\tRESULT EPOCH 21:\n",
      "\t\t\tTrain Loss: 0.02599175795222874 - Train Accuracy: 0.7280092592592593\n",
      "\n",
      "\tSTARTING EPOCH 22 - LR=[2]...\n",
      "\t\tTrain step - Step 1140, Loss 0.026553940027952194\n",
      "\t\tTrain step - Step 1170, Loss 0.02549091726541519\n",
      "\t\tRESULT EPOCH 22:\n",
      "\t\t\tTrain Loss: 0.02613482927834546 - Train Accuracy: 0.7285879629629629\n",
      "\n",
      "\tSTARTING EPOCH 23 - LR=[2]...\n",
      "\t\tTrain step - Step 1200, Loss 0.025408664718270302\n",
      "\t\tTrain step - Step 1230, Loss 0.025996346026659012\n",
      "\t\tRESULT EPOCH 23:\n",
      "\t\t\tTrain Loss: 0.025957278527871327 - Train Accuracy: 0.7403067129629629\n",
      "\n",
      "\tSTARTING EPOCH 24 - LR=[2]...\n",
      "\t\tTrain step - Step 1260, Loss 0.026148326694965363\n",
      "\t\tTrain step - Step 1290, Loss 0.028789959847927094\n",
      "\t\tRESULT EPOCH 24:\n",
      "\t\t\tTrain Loss: 0.02574689879461571 - Train Accuracy: 0.7436342592592593\n",
      "\n",
      "\tSTARTING EPOCH 25 - LR=[2]...\n",
      "\t\tTrain step - Step 1320, Loss 0.024666020646691322\n",
      "\t\tRESULT EPOCH 25:\n",
      "\t\t\tTrain Loss: 0.0255854442646658 - Train Accuracy: 0.7527488425925926\n",
      "\n",
      "\tSTARTING EPOCH 26 - LR=[2]...\n",
      "\t\tTrain step - Step 1350, Loss 0.024244045838713646\n",
      "\t\tTrain step - Step 1380, Loss 0.02622007392346859\n",
      "\t\tRESULT EPOCH 26:\n",
      "\t\t\tTrain Loss: 0.025346760914005614 - Train Accuracy: 0.7585358796296297\n",
      "\n",
      "\tSTARTING EPOCH 27 - LR=[2]...\n",
      "\t\tTrain step - Step 1410, Loss 0.02663881704211235\n",
      "\t\tTrain step - Step 1440, Loss 0.024837512522935867\n",
      "\t\tRESULT EPOCH 27:\n",
      "\t\t\tTrain Loss: 0.025628811610793625 - Train Accuracy: 0.7578125\n",
      "\n",
      "\tSTARTING EPOCH 28 - LR=[2]...\n",
      "\t\tTrain step - Step 1470, Loss 0.02422558329999447\n",
      "\t\tTrain step - Step 1500, Loss 0.02762562781572342\n",
      "\t\tRESULT EPOCH 28:\n",
      "\t\t\tTrain Loss: 0.0253612552597015 - Train Accuracy: 0.7611400462962963\n",
      "\n",
      "\tSTARTING EPOCH 29 - LR=[2]...\n",
      "\t\tTrain step - Step 1530, Loss 0.02384248375892639\n",
      "\t\tTrain step - Step 1560, Loss 0.02751843072474003\n",
      "\t\tRESULT EPOCH 29:\n",
      "\t\t\tTrain Loss: 0.025117605614165466 - Train Accuracy: 0.7744502314814815\n",
      "\n",
      "\tSTARTING EPOCH 30 - LR=[2]...\n",
      "\t\tTrain step - Step 1590, Loss 0.02524789236485958\n",
      "\t\tRESULT EPOCH 30:\n",
      "\t\t\tTrain Loss: 0.02542223881378218 - Train Accuracy: 0.7712673611111112\n",
      "\n",
      "\tSTARTING EPOCH 31 - LR=[2]...\n",
      "\t\tTrain step - Step 1620, Loss 0.024055251851677895\n",
      "\t\tTrain step - Step 1650, Loss 0.023371243849396706\n",
      "\t\tRESULT EPOCH 31:\n",
      "\t\t\tTrain Loss: 0.024909565087269853 - Train Accuracy: 0.7730034722222222\n",
      "\n",
      "\tSTARTING EPOCH 32 - LR=[2]...\n",
      "\t\tTrain step - Step 1680, Loss 0.025407500565052032\n",
      "\t\tTrain step - Step 1710, Loss 0.026206258684396744\n",
      "\t\tRESULT EPOCH 32:\n",
      "\t\t\tTrain Loss: 0.02481671215759383 - Train Accuracy: 0.7815393518518519\n",
      "\n",
      "\tSTARTING EPOCH 33 - LR=[2]...\n",
      "\t\tTrain step - Step 1740, Loss 0.024773716926574707\n",
      "\t\tTrain step - Step 1770, Loss 0.02420683577656746\n",
      "\t\tRESULT EPOCH 33:\n",
      "\t\t\tTrain Loss: 0.024672641315393977 - Train Accuracy: 0.78515625\n",
      "\n",
      "\tSTARTING EPOCH 34 - LR=[2]...\n",
      "\t\tTrain step - Step 1800, Loss 0.02513006702065468\n",
      "\t\tTrain step - Step 1830, Loss 0.02566491812467575\n",
      "\t\tRESULT EPOCH 34:\n",
      "\t\t\tTrain Loss: 0.024266001009554776 - Train Accuracy: 0.7889178240740741\n",
      "\n",
      "\tSTARTING EPOCH 35 - LR=[2]...\n",
      "\t\tTrain step - Step 1860, Loss 0.02598322369158268\n",
      "\t\tRESULT EPOCH 35:\n",
      "\t\t\tTrain Loss: 0.024204105897634116 - Train Accuracy: 0.7967303240740741\n",
      "\n",
      "\tSTARTING EPOCH 36 - LR=[2]...\n",
      "\t\tTrain step - Step 1890, Loss 0.02178020589053631\n",
      "\t\tTrain step - Step 1920, Loss 0.024398323148489\n",
      "\t\tRESULT EPOCH 36:\n",
      "\t\t\tTrain Loss: 0.024291768808055808 - Train Accuracy: 0.7974537037037037\n",
      "\n",
      "\tSTARTING EPOCH 37 - LR=[2]...\n",
      "\t\tTrain step - Step 1950, Loss 0.023243287578225136\n",
      "\t\tTrain step - Step 1980, Loss 0.0258269514888525\n",
      "\t\tRESULT EPOCH 37:\n",
      "\t\t\tTrain Loss: 0.024709006495497846 - Train Accuracy: 0.7912326388888888\n",
      "\n",
      "\tSTARTING EPOCH 38 - LR=[2]...\n",
      "\t\tTrain step - Step 2010, Loss 0.023423509672284126\n",
      "\t\tTrain step - Step 2040, Loss 0.024472517892718315\n",
      "\t\tRESULT EPOCH 38:\n",
      "\t\t\tTrain Loss: 0.024394784985041176 - Train Accuracy: 0.7973090277777778\n",
      "\n",
      "\tSTARTING EPOCH 39 - LR=[2]...\n",
      "\t\tTrain step - Step 2070, Loss 0.024980254471302032\n",
      "\t\tTrain step - Step 2100, Loss 0.022582320496439934\n",
      "\t\tRESULT EPOCH 39:\n",
      "\t\t\tTrain Loss: 0.024222283141204604 - Train Accuracy: 0.8035300925925926\n",
      "\n",
      "\tSTARTING EPOCH 40 - LR=[2]...\n",
      "\t\tTrain step - Step 2130, Loss 0.02204122208058834\n",
      "\t\tRESULT EPOCH 40:\n",
      "\t\t\tTrain Loss: 0.024289078217137744 - Train Accuracy: 0.8019386574074074\n",
      "\n",
      "\tSTARTING EPOCH 41 - LR=[2]...\n",
      "\t\tTrain step - Step 2160, Loss 0.02380954846739769\n",
      "\t\tTrain step - Step 2190, Loss 0.021200984716415405\n",
      "\t\tRESULT EPOCH 41:\n",
      "\t\t\tTrain Loss: 0.023915089356402557 - Train Accuracy: 0.8138020833333334\n",
      "\n",
      "\tSTARTING EPOCH 42 - LR=[2]...\n",
      "\t\tTrain step - Step 2220, Loss 0.022245530039072037\n",
      "\t\tTrain step - Step 2250, Loss 0.023527435958385468\n",
      "\t\tRESULT EPOCH 42:\n",
      "\t\t\tTrain Loss: 0.024006996717717912 - Train Accuracy: 0.8093171296296297\n",
      "\n",
      "\tSTARTING EPOCH 43 - LR=[2]...\n",
      "\t\tTrain step - Step 2280, Loss 0.02305205725133419\n",
      "\t\tTrain step - Step 2310, Loss 0.022766266018152237\n",
      "\t\tRESULT EPOCH 43:\n",
      "\t\t\tTrain Loss: 0.02377847706278165 - Train Accuracy: 0.8188657407407407\n",
      "\n",
      "\tSTARTING EPOCH 44 - LR=[2]...\n",
      "\t\tTrain step - Step 2340, Loss 0.020980292931199074\n",
      "\t\tTrain step - Step 2370, Loss 0.02462136186659336\n",
      "\t\tRESULT EPOCH 44:\n",
      "\t\t\tTrain Loss: 0.02362635221194338 - Train Accuracy: 0.8192997685185185\n",
      "\n",
      "\tSTARTING EPOCH 45 - LR=[2]...\n",
      "\t\tTrain step - Step 2400, Loss 0.02399597316980362\n",
      "\t\tRESULT EPOCH 45:\n",
      "\t\t\tTrain Loss: 0.023517610574210132 - Train Accuracy: 0.8214699074074074\n",
      "\n",
      "\tSTARTING EPOCH 46 - LR=[2]...\n",
      "\t\tTrain step - Step 2430, Loss 0.024037644267082214\n",
      "\t\tTrain step - Step 2460, Loss 0.024223465472459793\n",
      "\t\tRESULT EPOCH 46:\n",
      "\t\t\tTrain Loss: 0.023554491396579478 - Train Accuracy: 0.8232060185185185\n",
      "\n",
      "\tSTARTING EPOCH 47 - LR=[2]...\n",
      "\t\tTrain step - Step 2490, Loss 0.024628153070807457\n",
      "\t\tTrain step - Step 2520, Loss 0.024157200008630753\n",
      "\t\tRESULT EPOCH 47:\n",
      "\t\t\tTrain Loss: 0.023076442898147635 - Train Accuracy: 0.8327546296296297\n",
      "\n",
      "\tSTARTING EPOCH 48 - LR=[2]...\n",
      "\t\tTrain step - Step 2550, Loss 0.022337092086672783\n",
      "\t\tTrain step - Step 2580, Loss 0.022402068600058556\n",
      "\t\tRESULT EPOCH 48:\n",
      "\t\t\tTrain Loss: 0.023495451685179164 - Train Accuracy: 0.8310185185185185\n",
      "\n",
      "\tSTARTING EPOCH 49 - LR=[2]...\n",
      "\t\tTrain step - Step 2610, Loss 0.024674205109477043\n",
      "\t\tTrain step - Step 2640, Loss 0.022510649636387825\n",
      "\t\tRESULT EPOCH 49:\n",
      "\t\t\tTrain Loss: 0.023829305544495583 - Train Accuracy: 0.8274016203703703\n",
      "\n",
      "\tSTARTING EPOCH 50 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2670, Loss 0.023141469806432724\n",
      "\t\tRESULT EPOCH 50:\n",
      "\t\t\tTrain Loss: 0.021304290948642626 - Train Accuracy: 0.8556134259259259\n",
      "\n",
      "\tSTARTING EPOCH 51 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2700, Loss 0.020052239298820496\n",
      "\t\tTrain step - Step 2730, Loss 0.019547348842024803\n",
      "\t\tRESULT EPOCH 51:\n",
      "\t\t\tTrain Loss: 0.02004513930943277 - Train Accuracy: 0.8817997685185185\n",
      "\n",
      "\tSTARTING EPOCH 52 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2760, Loss 0.02072914130985737\n",
      "\t\tTrain step - Step 2790, Loss 0.019991068169474602\n",
      "\t\tRESULT EPOCH 52:\n",
      "\t\t\tTrain Loss: 0.019540107326099166 - Train Accuracy: 0.8799189814814815\n",
      "\n",
      "\tSTARTING EPOCH 53 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2820, Loss 0.017972247675061226\n",
      "\t\tTrain step - Step 2850, Loss 0.017905786633491516\n",
      "\t\tRESULT EPOCH 53:\n",
      "\t\t\tTrain Loss: 0.01935214525157655 - Train Accuracy: 0.8877314814814815\n",
      "\n",
      "\tSTARTING EPOCH 54 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2880, Loss 0.018657837063074112\n",
      "\t\tTrain step - Step 2910, Loss 0.01906820572912693\n",
      "\t\tRESULT EPOCH 54:\n",
      "\t\t\tTrain Loss: 0.01909846719354391 - Train Accuracy: 0.8913483796296297\n",
      "\n",
      "\tSTARTING EPOCH 55 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2940, Loss 0.019787654280662537\n",
      "\t\tRESULT EPOCH 55:\n",
      "\t\t\tTrain Loss: 0.019018721490822458 - Train Accuracy: 0.8903356481481481\n",
      "\n",
      "\tSTARTING EPOCH 56 - LR=[0.4]...\n",
      "\t\tTrain step - Step 2970, Loss 0.018189845606684685\n",
      "\t\tTrain step - Step 3000, Loss 0.01867624931037426\n",
      "\t\tRESULT EPOCH 56:\n",
      "\t\t\tTrain Loss: 0.018890107016044634 - Train Accuracy: 0.8955439814814815\n",
      "\n",
      "\tSTARTING EPOCH 57 - LR=[0.4]...\n",
      "\t\tTrain step - Step 3030, Loss 0.01928435079753399\n",
      "\t\tTrain step - Step 3060, Loss 0.020643562078475952\n",
      "\t\tRESULT EPOCH 57:\n",
      "\t\t\tTrain Loss: 0.01882745594614082 - Train Accuracy: 0.8984375\n",
      "\n",
      "\tSTARTING EPOCH 58 - LR=[0.4]...\n",
      "\t\tTrain step - Step 3090, Loss 0.018130624666810036\n",
      "\t\tTrain step - Step 3120, Loss 0.019079312682151794\n",
      "\t\tRESULT EPOCH 58:\n",
      "\t\t\tTrain Loss: 0.01872755787162869 - Train Accuracy: 0.8964120370370371\n",
      "\n",
      "\tSTARTING EPOCH 59 - LR=[0.4]...\n",
      "\t\tTrain step - Step 3150, Loss 0.01909392699599266\n",
      "\t\tTrain step - Step 3180, Loss 0.01949264667928219\n",
      "\t\tRESULT EPOCH 59:\n",
      "\t\t\tTrain Loss: 0.018591654831888498 - Train Accuracy: 0.8946759259259259\n",
      "\n",
      "\tSTARTING EPOCH 60 - LR=[0.4]...\n",
      "\t\tTrain step - Step 3210, Loss 0.01788902096450329\n",
      "\t\tRESULT EPOCH 60:\n",
      "\t\t\tTrain Loss: 0.018649133439693186 - Train Accuracy: 0.8977141203703703\n",
      "\n",
      "\tSTARTING EPOCH 61 - LR=[0.4]...\n",
      "\t\tTrain step - Step 3240, Loss 0.019331978633999825\n",
      "\t\tTrain step - Step 3270, Loss 0.019174201413989067\n",
      "\t\tRESULT EPOCH 61:\n",
      "\t\t\tTrain Loss: 0.018464942876663473 - Train Accuracy: 0.8935185185185185\n",
      "\n",
      "\tSTARTING EPOCH 62 - LR=[0.4]...\n",
      "\t\tTrain step - Step 3300, Loss 0.019476763904094696\n",
      "\t\tTrain step - Step 3330, Loss 0.01848003827035427\n",
      "\t\tRESULT EPOCH 62:\n",
      "\t\t\tTrain Loss: 0.018481374407807987 - Train Accuracy: 0.8987268518518519\n",
      "\n",
      "\tSTARTING EPOCH 63 - LR=[0.4]...\n",
      "\t\tTrain step - Step 3360, Loss 0.017822667956352234\n",
      "\t\tTrain step - Step 3390, Loss 0.01874425821006298\n",
      "\t\tRESULT EPOCH 63:\n",
      "\t\t\tTrain Loss: 0.018328330858989998 - Train Accuracy: 0.8951099537037037\n",
      "\n",
      "\tSTARTING EPOCH 64 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 3420, Loss 0.017762532457709312\n",
      "\t\tTrain step - Step 3450, Loss 0.018913540989160538\n",
      "\t\tRESULT EPOCH 64:\n",
      "\t\t\tTrain Loss: 0.018244590610265732 - Train Accuracy: 0.9058159722222222\n",
      "\n",
      "\tSTARTING EPOCH 65 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 3480, Loss 0.01894526369869709\n",
      "\t\tRESULT EPOCH 65:\n",
      "\t\t\tTrain Loss: 0.018150560074934253 - Train Accuracy: 0.9056712962962963\n",
      "\n",
      "\tSTARTING EPOCH 66 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 3510, Loss 0.01739347353577614\n",
      "\t\tTrain step - Step 3540, Loss 0.020219041034579277\n",
      "\t\tRESULT EPOCH 66:\n",
      "\t\t\tTrain Loss: 0.01801155735221174 - Train Accuracy: 0.9076967592592593\n",
      "\n",
      "\tSTARTING EPOCH 67 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 3570, Loss 0.017220942303538322\n",
      "\t\tTrain step - Step 3600, Loss 0.01657378114759922\n",
      "\t\tRESULT EPOCH 67:\n",
      "\t\t\tTrain Loss: 0.017991771438607463 - Train Accuracy: 0.9088541666666666\n",
      "\n",
      "\tSTARTING EPOCH 68 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 3630, Loss 0.01733287423849106\n",
      "\t\tTrain step - Step 3660, Loss 0.018510129302740097\n",
      "\t\tRESULT EPOCH 68:\n",
      "\t\t\tTrain Loss: 0.017955333408382203 - Train Accuracy: 0.9058159722222222\n",
      "\n",
      "\tSTARTING EPOCH 69 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 3690, Loss 0.01803719811141491\n",
      "\t\tTrain step - Step 3720, Loss 0.017291570082306862\n",
      "\t\tRESULT EPOCH 69:\n",
      "\t\t\tTrain Loss: 0.017986918316671142 - Train Accuracy: 0.9091435185185185\n",
      "\n",
      "\tSTARTING EPOCH 70 - LR=[0.08000000000000002]...\n",
      "\t\tTrain step - Step 3750, Loss 0.018453145399689674\n",
      "\t\tRESULT EPOCH 70:\n",
      "\t\t\tTrain Loss: 0.018040707541836634 - Train Accuracy: 0.9049479166666666\n",
      "\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\r  0%|          | 0/24 [00:00<?, ?it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "TEEEEEEEEEEEEST\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:01<00:00, 18.83it/s]"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\n",
      "\tResults STAGE 3:\n",
      "\t\tTrain Mean Accuracy: 0.7636801421957672\n",
      "\t\tTest Accuracy: 0.6576666666666666\n",
      "\n",
      "STARTING FINE TUNING STAGE 4...\n",
      "[37732, 509, 509, 2020, 57, 9587, 118, 14710, 8023, 2020, 14710, 14710, 57, 57, 57, 57, 509, 14710, 57, 14710, 57, 9587, 57, 9587, 509, 509, 57, 10234, 57, 9587, 57, 57, 9587, 9587, 57, 9587, 10234, 57, 509, 57, 57, 9587, 9587, 57, 4623, 57, 4623, 9587, 9587, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 33643, 3709, 449, 449, 449, 449, 449, 985, 985, 3636, 48697, 449, 449, 3620, 449, 3620, 3620, 3636, 3620, 3620, 30006, 449, 313, 313, 36355, 372, 313, 313, 449, 36355, 36355, 36355, 449, 313, 313, 313, 1532, 313, 313, 313, 1532, 36355, 313, 1532, 313, 313, 36355, 313, 313, 1532, 36355, 313, 313, 313, 313, 313, 313, 36355, 313, 313, 1532, 313, 313, 313, 313, 313, 313, 43149, 8949, 48806, 8949, 911, 48806, 120, 120, 13416, 120, 120, 439, 120, 120, 120, 120, 1292, 120, 13416, 1292, 120, 120, 13416, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 1292, 120, 120, 120, 1292, 1292, 120, 1292, 120, 1292, 1292, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 120, 27295, 2057, 6943, 1726, 27, 27, 27, 27, 27, 27, 7172, 1726, 1726, 1726, 14145, 27, 27, 27, 27, 27, 27, 27, 27, 27, 8302, 27, 27, 27, 27, 4661, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 24723, 2852, 5375, 219, 219, 31381, 219, 9101, 31381, 16, 575, 575, 16, 16, 16, 16, 16, 4221, 5375, 16, 7101, 16, 16, 16, 219, 16, 16, 219, 5375, 1255, 16, 16, 16, 16, 7101, 16, 16, 16, 16, 219, 219, 219, 219, 16, 219, 219, 16, 16, 219, 16, 16, 219, 16, 219, 16, 16, 219, 31381, 16, 16, 16, 16, 219, 219, 219, 16, 219, 29770, 25520, 148, 12106, 23617, 148, 148, 148, 148, 36079, 148, 20, 536, 148, 536, 148, 148, 148, 148, 148, 148, 148, 665, 25520, 148, 665, 665, 148, 148, 25520, 665, 20, 25520, 665, 20, 20, 20, 20, 20, 20, 25520, 665, 665, 20, 20, 25520, 148, 20, 148, 20, 20, 20, 20, 20, 20, 148, 148, 20, 148, 20, 148, 20, 20, 20, 20, 20, 20, 13806, 1558, 6938, 1558, 6938, 1558, 6938, 172, 1558, 24295, 43986, 34, 172, 172, 34, 172, 34, 172, 34, 24295, 1558, 172, 1558, 34, 172, 25397, 25397, 25397, 34, 25397, 25397, 34, 34, 34, 34, 172, 172, 172, 172, 172, 34, 172, 172, 172, 172, 34, 34, 34, 34, 34, 172, 34, 34, 172, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 172, 34, 44516, 7163, 4056, 124, 4614, 1789, 4056, 2774, 4056, 4056, 14205, 14205, 26774, 8287, 43171, 4056, 124, 124, 124, 124, 124, 124, 8287, 124, 43171, 124, 124, 531, 124, 531, 124, 124, 124, 124, 124, 124, 410, 124, 124, 410, 124, 124, 124, 4056, 124, 124, 531, 7163, 124, 124, 124, 531, 124, 124, 124, 124, 124, 124, 124, 124, 124, 124, 410, 43171, 124, 124, 124, 45486, 33873, 3768, 18, 18, 18, 18, 18, 33873, 18, 18, 18, 18, 192, 18, 20037, 33873, 1906, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 1906, 1906, 1906, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 10458, 10249, 17162, 8519, 555, 17162, 76, 76, 76, 76, 76, 73, 10249, 76, 76, 76, 76, 76, 76, 76, 76, 1791, 76, 1791, 76, 76, 1791, 3741, 3741, 3741, 76, 3741, 12714, 3741, 12714, 3741, 3741, 76, 76, 73, 76, 76, 76, 76, 48, 76, 76, 76, 10249, 76, 76, 76, 1791, 1791, 48, 1791, 76, 3741, 76, 3741, 76, 3741, 10249, 76, 76, 76, 76, 11449, 25771, 25771, 12, 12, 274, 274, 12, 12, 25771, 12, 466, 7442, 9746, 9746, 9746, 7442, 16784, 43293, 164, 12, 12, 12, 43293, 12, 8573, 16784, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 274, 12, 12, 12, 274, 12, 274, 12, 12, 12, 12, 9746, 12, 12, 12, 12, 12, 12, 12, 164, 164, 12, 12, 12, 12, 12, 12, 12, 12, 12, 37752, 14789, 243, 476, 14789, 1193, 6184, 1113, 1113, 14789, 243, 243, 243, 956, 956, 956, 956, 13949, 14094, 83, 1438, 83, 1438, 243, 243, 83, 83, 83, 83, 243, 243, 956, 14789, 83, 83, 83, 476, 476, 476, 476, 83, 83, 83, 476, 476, 83, 476, 83, 83, 83, 83, 83, 83, 83, 83, 83, 243, 117, 243, 83, 83, 83, 117, 117, 83, 83, 117, 46840, 6389, 658, 304, 789, 789, 12738, 176, 176, 3458, 176, 176, 176, 176, 176, 176, 42834, 9470, 2, 176, 176, 2, 176, 17036, 2, 2, 32159, 2, 2, 2, 2, 2, 2, 3007, 2, 2, 2, 2, 2, 2, 2, 176, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3007, 3007, 24028, 8342, 17033, 29624, 634, 634, 634, 29624, 6676, 87, 87, 8342, 87, 634, 87, 634, 8342, 87, 634, 87, 87, 87, 1692, 87, 87, 8342, 28085, 87, 87, 87, 87, 87, 87, 991, 87, 87, 87, 634, 87, 28085, 991, 87, 87, 634, 87, 655, 87, 87, 87, 634, 87, 87, 4230, 87, 1692, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 87, 42309, 18459, 10580, 5000, 79, 79, 4030, 949, 79, 79, 29861, 79, 79, 79, 79, 28, 79, 28, 79, 79, 949, 79, 79, 79, 17331, 17331, 28, 28, 28, 28, 28, 28, 28, 79, 28, 28, 79, 28, 28, 28, 28, 28, 28, 28, 28, 79, 79, 79, 28, 28, 28, 28, 28, 28, 18792, 79, 79, 28, 28, 28, 28, 18792, 79, 79, 28, 28, 28, 35415, 33228, 426, 394, 191, 426, 426, 6455, 2742, 191, 191, 426, 1392, 426, 191, 394, 6455, 125, 34332, 125, 13963, 125, 125, 125, 125, 2974, 125, 125, 2974, 125, 4863, 2974, 125, 125, 125, 125, 125, 125, 125, 125, 125, 8323, 426, 715, 8323, 715, 426, 125, 125, 125, 125, 125, 125, 125, 125, 715, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 125, 6449, 37234, 717, 2116, 37234, 717, 717, 116, 1673, 116, 415, 415, 717, 415, 717, 1855, 279, 279, 1673, 932, 116, 209, 116, 415, 717, 717, 717, 415, 717, 1185, 717, 717, 8173, 116, 1185, 1185, 116, 2450, 2450, 116, 717, 717, 116, 116, 116, 116, 37234, 116, 320, 116, 320, 116, 116, 116, 116, 116, 116, 415, 415, 116, 116, 116, 116, 415, 415, 116, 42651, 12204, 7021, 7021, 7021, 7021, 751, 1547, 1547, 1547, 1547, 6672, 7021, 7021, 6044, 7021, 419, 419, 650, 419, 419, 4227, 650, 13629, 3279, 419, 1547, 70, 70, 13629, 70, 70, 70, 3279, 650, 70, 1547, 70, 1547, 70, 70, 1547, 70, 650, 6044, 751, 70, 751, 1547, 70, 70, 70, 1547, 1547, 1547, 70, 7021, 7021, 1056, 1112, 1547, 1112, 1112, 7021, 1112, 1112, 1547, 1547, 36150, 252, 7976, 252, 252, 252, 252, 252, 576, 252, 7976, 576, 576, 1841, 2499, 11975, 152, 576, 576, 195, 195, 195, 576, 152, 46724, 152, 251, 152, 152, 152, 152, 152, 152, 408, 1965, 152, 252, 152, 152, 251, 252, 152, 3817, 152, 152, 251, 408, 251, 251, 251, 251, 251, 152, 251, 251, 152, 152, 251, 152, 251, 251, 251, 251, 251, 152, 152, 251, 385, 37570, 4908, 21167, 889, 307, 30225, 244, 244, 4908, 244, 244, 244, 7443, 244, 244, 244, 244, 244, 244, 4908, 244, 9386, 12266, 244, 244, 244, 7443, 7443, 7443, 7443, 7443, 7443, 244, 7443, 244, 244, 244, 7443, 7443, 307, 244, 244, 7443, 244, 244, 7443, 244, 7443, 244, 244, 244, 7443, 244, 7443, 244, 7443, 244, 244, 7443, 307, 244, 7443, 244, 244, 244, 244, 37405, 2064, 2064, 2064, 2064, 10471, 10471, 1151, 1829, 2064, 1204, 2064, 1204, 2064, 2064, 1151, 1829, 1829, 757, 1224, 2064, 2064, 2064, 2064, 34353, 2064, 757, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 2064, 99, 99, 99, 757, 99, 99, 99, 99, 99, 99, 99, 99, 16641, 45163, 4851, 28527, 113, 113, 638, 10983, 10983, 10983, 102, 102, 102, 102, 102, 4813, 4813, 102, 4813, 4813, 102, 102, 102, 1148, 102, 638, 36241, 102, 102, 102, 102, 102, 102, 638, 102, 638, 389, 102, 638, 102, 638, 22274, 102, 102, 102, 102, 102, 102, 102, 113, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 102, 8460, 41284, 4778, 2635, 6984, 2381, 6984, 253, 379, 7045, 7045, 46371, 1209, 1209, 4325, 586, 1209, 1209, 1209, 1209, 8334, 64, 1209, 1209, 4325, 64, 64, 1209, 64, 8334, 64, 64, 8334, 8334, 8334, 64, 8334, 586, 8334, 64, 8334, 8334, 8334, 64, 8334, 64, 64, 586, 8334, 64, 8334, 64, 64, 64, 64, 4778, 4778, 64, 64, 64, 4778, 64, 64, 64, 64, 64, 41758, 23550, 23550, 175, 175, 175, 28803, 13661, 171, 171, 62, 16252, 62, 171, 16252, 33599, 62, 62, 62, 171, 62, 171, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 16252, 62, 62, 16252, 62, 62, 16252, 62, 62, 16252, 175, 62, 175, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 62, 175, 62, 62, 175, 62, 175, 62, 175, 62, 175, 22143, 16398, 2998, 3441, 2998, 23285, 271, 271, 271, 1396, 271, 271, 271, 6483, 1017, 1017, 1017, 23285, 960, 271, 16398, 41141, 271, 3441, 4, 271, 4, 4, 4, 4, 271, 3441, 3441, 3441, 3441, 271, 271, 3441, 3553, 4, 4, 3441, 3441, 4, 4, 4, 3441, 4, 4, 4, 271, 3441, 4, 4, 4, 4, 4, 3441, 4, 4, 4, 4, 4, 4, 4, 4, 6034, 24931, 24931, 139, 2008, 1200, 139, 2008, 2996, 139, 2150, 13394, 2150, 23, 23, 132, 132, 139, 2008, 16920, 2416, 1748, 2008, 1748, 139, 132, 23, 23, 23, 1748, 23, 23, 1748, 23, 23, 23, 23, 2150, 23, 23, 23, 139, 132, 7609, 139, 2008, 3115, 132, 132, 139, 132, 7609, 132, 3115, 132, 3115, 2615, 132, 132, 2615, 2615, 2615, 2615, 132, 132, 132, 27737, 5214, 7482, 21283, 40, 4655, 5214, 5214, 5214, 5214, 40, 40, 40, 40, 40, 40, 5214, 5214, 5214, 40, 4655, 5214, 4655, 40, 21283, 305, 5214, 5214, 5214, 5214, 5214, 5214, 305, 5214, 5214, 305, 305, 40, 40, 19034, 40, 40, 305, 5214, 305, 21283, 40, 5214, 19034, 5214, 19034, 40, 40, 40, 40, 40, 40, 305, 40, 49, 40, 40, 40, 49, 40, 40, 42655, 1581, 32381, 1718, 1718, 38, 38, 7401, 1087, 4308, 24740, 24740, 38, 902, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 1718, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 1718, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 1718, 38, 38, 38, 38, 38, 38, 38, 38, 13756, 3786, 373, 13375, 19, 19, 373, 19, 19, 19, 19, 373, 18569, 3786, 9925, 3786, 47640, 19, 19, 14, 19, 14, 19, 13375, 2144, 14, 54, 14, 14, 16385, 14, 14, 14, 54, 14, 14, 16385, 373, 16385, 14, 14, 14, 14, 14, 14, 14, 54, 373, 25600, 373, 14, 373, 14, 14, 14, 16385, 373, 54, 373, 16385, 16385, 14, 373, 373, 14, 373, 35372, 246, 31723, 12983, 246, 167, 167, 246, 167, 246, 167, 167, 167, 225, 167, 246, 167, 167, 167, 246, 167, 15935, 246, 167, 167, 167, 167, 246, 167, 246, 167, 167, 167, 246, 167, 167, 167, 246, 246, 167, 246, 167, 246, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 167, 246, 167, 167, 167, 167]\n",
      "2000\n",
      "\tSTARTING EPOCH 1 - LR=[2]...\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "\t\tTrain step - Step 30, Loss 0.04511108249425888\n",
      "\t\tRESULT EPOCH 1:\n",
      "\t\t\tTrain Loss: 0.052163619410108636 - Train Accuracy: 0.2795138888888889\n",
      "\n",
      "\tSTARTING EPOCH 2 - LR=[2]...\n",
      "\t\tTrain step - Step 60, Loss 0.04469309747219086\n",
      "\t\tTrain step - Step 90, Loss 0.04179792478680611\n",
      "\t\tRESULT EPOCH 2:\n",
      "\t\t\tTrain Loss: 0.04294356538189782 - Train Accuracy: 0.36863425925925924\n",
      "\n",
      "\tSTARTING EPOCH 3 - LR=[2]...\n",
      "\t\tTrain step - Step 120, Loss 0.03993283584713936\n",
      "\t\tTrain step - Step 150, Loss 0.041019681841135025\n",
      "\t\tRESULT EPOCH 3:\n",
      "\t\t\tTrain Loss: 0.0410098677708043 - Train Accuracy: 0.44545717592592593\n",
      "\n",
      "\tSTARTING EPOCH 4 - LR=[2]...\n",
      "\t\tTrain step - Step 180, Loss 0.03996425122022629\n",
      "\t\tTrain step - Step 210, Loss 0.03843278810381889\n",
      "\t\tRESULT EPOCH 4:\n",
      "\t\t\tTrain Loss: 0.03996533245124199 - Train Accuracy: 0.49291087962962965\n",
      "\n",
      "\tSTARTING EPOCH 5 - LR=[2]...\n",
      "\t\tTrain step - Step 240, Loss 0.039608433842659\n",
      "\t\tRESULT EPOCH 5:\n",
      "\t\t\tTrain Loss: 0.039300631938709155 - Train Accuracy: 0.5209780092592593\n",
      "\n",
      "\tSTARTING EPOCH 6 - LR=[2]...\n",
      "\t\tTrain step - Step 270, Loss 0.03744005039334297\n",
      "\t\tTrain step - Step 300, Loss 0.03662760555744171\n",
      "\t\tRESULT EPOCH 6:\n",
      "\t\t\tTrain Loss: 0.038975630300464456 - Train Accuracy: 0.5506365740740741\n",
      "\n",
      "\tSTARTING EPOCH 7 - LR=[2]...\n",
      "\t\tTrain step - Step 330, Loss 0.03690158203244209\n",
      "\t\tTrain step - Step 360, Loss 0.035894617438316345\n",
      "\t\tRESULT EPOCH 7:\n",
      "\t\t\tTrain Loss: 0.038689414108241046 - Train Accuracy: 0.5713252314814815\n",
      "\n",
      "\tSTARTING EPOCH 8 - LR=[2]...\n",
      "\t\tTrain step - Step 390, Loss 0.03980742767453194\n",
      "\t\tTrain step - Step 420, Loss 0.03847622871398926\n",
      "\t\tRESULT EPOCH 8:\n",
      "\t\t\tTrain Loss: 0.0380508107719598 - Train Accuracy: 0.5813078703703703\n",
      "\n",
      "\tSTARTING EPOCH 9 - LR=[2]...\n",
      "\t\tTrain step - Step 450, Loss 0.03734173625707626\n",
      "\t\tTrain step - Step 480, Loss 0.03812575340270996\n",
      "\t\tRESULT EPOCH 9:\n",
      "\t\t\tTrain Loss: 0.038104438257438165 - Train Accuracy: 0.6077835648148148\n",
      "\n",
      "\tSTARTING EPOCH 10 - LR=[2]...\n",
      "\t\tTrain step - Step 510, Loss 0.03726910799741745\n",
      "\t\tRESULT EPOCH 10:\n",
      "\t\t\tTrain Loss: 0.03741463522116343 - Train Accuracy: 0.6232638888888888\n",
      "\n",
      "\tSTARTING EPOCH 11 - LR=[2]...\n",
      "\t\tTrain step - Step 540, Loss 0.03704763576388359\n",
      "\t\tTrain step - Step 570, Loss 0.03712133318185806\n",
      "\t\tRESULT EPOCH 11:\n",
      "\t\t\tTrain Loss: 0.03744862532174146 - Train Accuracy: 0.6339699074074074\n",
      "\n",
      "\tSTARTING EPOCH 12 - LR=[2]...\n",
      "\t\tTrain step - Step 600, Loss 0.03497323766350746\n",
      "\t\tTrain step - Step 630, Loss 0.03835581988096237\n",
      "\t\tRESULT EPOCH 12:\n",
      "\t\t\tTrain Loss: 0.03687348865248539 - Train Accuracy: 0.6591435185185185\n",
      "\n",
      "\tSTARTING EPOCH 13 - LR=[2]...\n",
      "\t\tTrain step - Step 660, Loss 0.03686899319291115\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-eb7df811badb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_accuracies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0micarl_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_val_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-72840b26edc0>\u001b[0m in \u001b[0;36micarl_training\u001b[0;34m(train_val_dataset, test_dataset, max_epoch, file_path, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\tSTARTING EPOCH {epoch+1} - LR={scheduler.get_last_lr()}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mcurr_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurr_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0mcurr_train_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mcurr_train_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0986c93881b7>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(net, train_loader, optimizer, current_step, device)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mrunning_corrects\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ciGlvEWabcbD",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "import libs.plots as plots\n",
    "\n",
    "method = \"icarl\"\n",
    "plots.plot_accuracy_trend(test_accuracies, method, SEED)\n",
    "plots.plot_confusion_matrix(y_true, y_preds, method, SEED)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "amC8Qy-yVw8W",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "def save_accuracies(train_accuracies, test_accuracies, output=OUTPUT_PATH):\n",
    "  with open(f\"{output}_accuracies.csv\", \"w\", encoding=\"utf8\") as f:\n",
    "    f.write(\"mean_train_acc,mean_val_acc,test_acc\\n\")\n",
    "    for train, test in zip(train_accuracies, test_accuracies):\n",
    "      f.write(f\"{train},{test}\\n\")\n",
    "    print(\"********** FILE SAVED **********\")\n",
    "\n",
    "\n",
    "save_accuracies(train_accuracies, test_accuracies)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}